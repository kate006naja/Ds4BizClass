{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Mining\n",
    "\n",
    "Term 1 2019 - Instructor: Teerapong Leelanupab\n",
    "\n",
    "Teaching Assistant: Suttida Satjasunsern\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we look at a range of text mining techniques, available as part of Scikit-learn.\n",
    "\n",
    "As our sample corpus of text, we will read a collection of news articles. These articles have been stored in a single file and formatted so that one article appears on each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 45 raw text documents\n"
     ]
    }
   ],
   "source": [
    "fin = open(\"data/news-articles.txt\",\"r\")\n",
    "raw_documents = fin.readlines()\n",
    "fin.close()\n",
    "print(\"Read %d raw text documents\" % len(raw_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw text documents are textual, not numeric. The first step in analysing unstructured documents is to split the raw text into individual tokens, each corresponding to a single term (word). As an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parry puts Gerrard 'above money' Listen to the full interview on Sport on Five and the BBC Sport website from 1900 GMT. But Parry, speaking exclusively to BBC Sport, also admits Gerrard, who has been constantly linked with Chelsea, will have the final say on his future. He told BBC Five Live: \"Steven is above money. He is the future of Liverpool. \"It doesn't matter if it's Â£30m, Â£40m or Â£50m, we will not accept offers. But we are also realistic enough to know we can't keep Steven against his will.\" On the subject of Liverpool's finances, Parry also revealed the club is ready to explore the possibility of a sponsorship deal for its proposed new stadium. And responding to criticism from BBC Sport pundit and former Liverpool stalwart Alan Hansen, he insisted talks on new investment are ongoing, but added the door has not closed on shareholder and lifelong fan Steve Morgan. Parry joined Liverpool as chief executive in July 1998 from a similar role at the Premier League. There have been several highs and lows during his time in charge at Anfield - and he had a busy summer, overseeing the arrival of new manager Rafael Benitez and managing to hold on to Steven Gerrard. On the subject of Liverpool's captain and prize asset, Parry revealed Real Madrid did ask for an option on the England midfield man during negotiations for striker Fernando Morientes. He said: \"They were looking for ways of saying they got more out of the deal for Fernando Morientes, but the response to Real Madrid was the same - Steven is not for sale.\" But when asked if Gerrard would be a Liverpool player on the first day of next season, Parry said: \"I sincerely hope he will be. Steven knows my views. He knows Rafa's views. \"We have re-affirmed recently to Steven that we are trying to build a team around him. We crave success as much as he does. We know he's ambitious and nobody can argue with that. \"I think Steven would dearly love to win things with Liverpool more than he'd like to do anything else. \"We all want to see progress by next season. He's not alone in that. There are a lot of other players who feel the same, so we all have a common aim.\" It is expected Chelsea will test Liverpool with a Â£30m-plus bid in the summer - but Parry claims he will be in no mood to listen. \"There have been a lot of open secrets about Steven, most of which have been complete myths. It is suggested we had a deal tied up last summer. We didn't had an offer last summer,\" Parry explained. \"We had told Chelsea that as far as we were concerned he was not for sale and we didn't want to sell him. In reality it didn't go beyond that. \"Maybe there will be an offer in the summer. Maybe there won't. \"Our position is we want Steven to stay, but we are also realistic enough and have enough respect for Steven - and he has enough respect for us - to know that it is his decision that will be crucial. \"You are not going to keep a player like Steven against his will. That just doesn't work, but any idea we are going to accept offers for Steven and then tell him 'by the way we've decided to sell you' is not on the agenda. You can forget that.\" Parry is currently in the process of finalising funding for Liverpool's new stadium in Stanley Park, which is set to open in 2007. And he confessed Arsenal's Â£100m deal with Emirates to sponsor their new ground - complete with naming rights - has given the Anfield club serious food for thought. He said: \"I have to say historically it is something I have been against, and I have been on record as saying that, but I think the size of the Arsenal deal is a real eye-opener. \"I would say in the past deals have been done frankly far too cheaply and it just hasn't even been worth contemplating. \"But the Arsenal deal is the sort of deal that causes you to draw breath and say 'wow - that's interesting.' \"My personal point of view is that I would find it a hell of a lot more palatable than a shared stadium.\" Some Liverpool fans would find such a move highly controversial, but Parry countered: \"I recognise it would be an emotive issue for many supporters, but you look at the amount of money available and it could go into the team. \"If it was the right partner how strong an issue is it? Time will tell. \"I think the stadium will always be Anfield, not least because of where it is, but do we need to investigate the possibilities of sponsorship? I think it would be remiss not to. \"That's not to say we have made a decision that we will go down that road, but I think it is clearly something we have to explore.\" On the subject of possible new investment, Parry revealed Liverpool are still in negotiations with a mystery investor, with rumours of interest from the Middle East. That prompted the withdrawal of tycoon Steve Morgan, who got frustrated by failed bids and what he claimed was indecision by the board. He also accused Liverpool of using him as \"a stalking horse\" to attract other bids, but Parry explained: \"Steve has never been used as a stalking horse. There's no need, and that is not the way we do business. \"We had discussions with Steve over the course of 2004. I think we came close to concluding a deal in the summer but it didn't happen. \"Quite genuinely, the new interest did appear relatively late in the day just prior to the AGM in December, and as I have said it was of such potential magnitude, and that potential is so exciting, we felt we had to evaluate it. We are still evaluating it. \"Steve's interest was taken very much on its own merits. His enthusiasm for the club is there for all to see and who knows what the next few months will hold? \"The door isn't closed on anything. We had a perfectly sensible dialogue with Steve last year. \"We have a common interest in making Liverpool successful. That's a dream we all share, so as far as I'm concerned the door is not closed.\" I would take Â£50m if we had no investment, but if we did, keep him. As for the stadium, if it gets us cash what difference does it make really? Â£50m for Gerrard? I don't care who you are, the Directors would take the money and it is the way it should be. We cannot let that sum of money go, despite Gerrard's quality. Through a cleverly worded statement, the club has effectively forced Gerrard to publicly make the decision for himself, which I think is the right thing to do. Critical time for Liverpool with regards to Gerrard. Ideally we would want to secure his future to the club for the long term. I am hoping he doesn't walk out of the club like Michael Owen did for very little cash. Â£50m realistically would allow Rafa to completely rebuild the squad, however, if we can afford to do this AND keep Gerrard we will be better for it. I would however be happy with Gerrard's transfer for any fee over Â£35m. Parry's statements are clever in that any future Gerrard transfer cannot be construed as a lack of ambition by the club to not try and keep their best players. Upping the ante is another smart move by Parry. I would keep Gerrard. No amount of money could replace his obvious love of the club and determination to succeed. The key is if Gerrard comes out and says that he is happy. Clearly, if he isn't, then we would be foolish not to sell. The worrying thing is who would you buy (or who would come) pending possible non-Champions League football.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc1 = raw_documents[0]\n",
    "# print a snippet\n",
    "print(doc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the built-in scikit-learn tokenizer to split this document into tokens. Note that we will perform *case conversion* first to convert the entire text to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['parry', 'puts', 'gerrard', 'above', 'money', 'listen', 'to', 'the', 'full', 'interview', 'on', 'sport', 'on', 'five', 'and', 'the', 'bbc', 'sport', 'website', 'from', '1900', 'gmt', 'but', 'parry', 'speaking', 'exclusively', 'to', 'bbc', 'sport', 'also', 'admits', 'gerrard', 'who', 'has', 'been', 'constantly', 'linked', 'with', 'chelsea', 'will', 'have', 'the', 'final', 'say', 'on', 'his', 'future', 'he', 'told', 'bbc', 'five', 'live', 'steven', 'is', 'above', 'money', 'he', 'is', 'the', 'future', 'of', 'liverpool', 'it', 'doesn', 'matter', 'if', 'it', '30m', '40m', 'or', '50m', 'we', 'will', 'not', 'accept', 'offers', 'but', 'we', 'are', 'also', 'realistic', 'enough', 'to', 'know', 'we', 'can', 'keep', 'steven', 'against', 'his', 'will', 'on', 'the', 'subject', 'of', 'liverpool', 'finances', 'parry', 'also', 'revealed', 'the', 'club', 'is', 'ready', 'to', 'explore', 'the', 'possibility', 'of', 'sponsorship', 'deal', 'for', 'its', 'proposed', 'new', 'stadium', 'and', 'responding', 'to', 'criticism', 'from', 'bbc', 'sport', 'pundit', 'and', 'former', 'liverpool', 'stalwart', 'alan', 'hansen', 'he', 'insisted', 'talks', 'on', 'new', 'investment', 'are', 'ongoing', 'but', 'added', 'the', 'door', 'has', 'not', 'closed', 'on', 'shareholder', 'and', 'lifelong', 'fan', 'steve', 'morgan', 'parry', 'joined', 'liverpool', 'as', 'chief', 'executive', 'in', 'july', '1998', 'from', 'similar', 'role', 'at', 'the', 'premier', 'league', 'there', 'have', 'been', 'several', 'highs', 'and', 'lows', 'during', 'his', 'time', 'in', 'charge', 'at', 'anfield', 'and', 'he', 'had', 'busy', 'summer', 'overseeing', 'the', 'arrival', 'of', 'new', 'manager', 'rafael', 'benitez', 'and', 'managing', 'to', 'hold', 'on', 'to', 'steven', 'gerrard', 'on', 'the', 'subject', 'of', 'liverpool', 'captain', 'and', 'prize', 'asset', 'parry', 'revealed', 'real', 'madrid', 'did', 'ask', 'for', 'an', 'option', 'on', 'the', 'england', 'midfield', 'man', 'during', 'negotiations', 'for', 'striker', 'fernando', 'morientes', 'he', 'said', 'they', 'were', 'looking', 'for', 'ways', 'of', 'saying', 'they', 'got', 'more', 'out', 'of', 'the', 'deal', 'for', 'fernando', 'morientes', 'but', 'the', 'response', 'to', 'real', 'madrid', 'was', 'the', 'same', 'steven', 'is', 'not', 'for', 'sale', 'but', 'when', 'asked', 'if', 'gerrard', 'would', 'be', 'liverpool', 'player', 'on', 'the', 'first', 'day', 'of', 'next', 'season', 'parry', 'said', 'sincerely', 'hope', 'he', 'will', 'be', 'steven', 'knows', 'my', 'views', 'he', 'knows', 'rafa', 'views', 'we', 'have', 're', 'affirmed', 'recently', 'to', 'steven', 'that', 'we', 'are', 'trying', 'to', 'build', 'team', 'around', 'him', 'we', 'crave', 'success', 'as', 'much', 'as', 'he', 'does', 'we', 'know', 'he', 'ambitious', 'and', 'nobody', 'can', 'argue', 'with', 'that', 'think', 'steven', 'would', 'dearly', 'love', 'to', 'win', 'things', 'with', 'liverpool', 'more', 'than', 'he', 'like', 'to', 'do', 'anything', 'else', 'we', 'all', 'want', 'to', 'see', 'progress', 'by', 'next', 'season', 'he', 'not', 'alone', 'in', 'that', 'there', 'are', 'lot', 'of', 'other', 'players', 'who', 'feel', 'the', 'same', 'so', 'we', 'all', 'have', 'common', 'aim', 'it', 'is', 'expected', 'chelsea', 'will', 'test', 'liverpool', 'with', '30m', 'plus', 'bid', 'in', 'the', 'summer', 'but', 'parry', 'claims', 'he', 'will', 'be', 'in', 'no', 'mood', 'to', 'listen', 'there', 'have', 'been', 'lot', 'of', 'open', 'secrets', 'about', 'steven', 'most', 'of', 'which', 'have', 'been', 'complete', 'myths', 'it', 'is', 'suggested', 'we', 'had', 'deal', 'tied', 'up', 'last', 'summer', 'we', 'didn', 'had', 'an', 'offer', 'last', 'summer', 'parry', 'explained', 'we', 'had', 'told', 'chelsea', 'that', 'as', 'far', 'as', 'we', 'were', 'concerned', 'he', 'was', 'not', 'for', 'sale', 'and', 'we', 'didn', 'want', 'to', 'sell', 'him', 'in', 'reality', 'it', 'didn', 'go', 'beyond', 'that', 'maybe', 'there', 'will', 'be', 'an', 'offer', 'in', 'the', 'summer', 'maybe', 'there', 'won', 'our', 'position', 'is', 'we', 'want', 'steven', 'to', 'stay', 'but', 'we', 'are', 'also', 'realistic', 'enough', 'and', 'have', 'enough', 'respect', 'for', 'steven', 'and', 'he', 'has', 'enough', 'respect', 'for', 'us', 'to', 'know', 'that', 'it', 'is', 'his', 'decision', 'that', 'will', 'be', 'crucial', 'you', 'are', 'not', 'going', 'to', 'keep', 'player', 'like', 'steven', 'against', 'his', 'will', 'that', 'just', 'doesn', 'work', 'but', 'any', 'idea', 'we', 'are', 'going', 'to', 'accept', 'offers', 'for', 'steven', 'and', 'then', 'tell', 'him', 'by', 'the', 'way', 'we', 've', 'decided', 'to', 'sell', 'you', 'is', 'not', 'on', 'the', 'agenda', 'you', 'can', 'forget', 'that', 'parry', 'is', 'currently', 'in', 'the', 'process', 'of', 'finalising', 'funding', 'for', 'liverpool', 'new', 'stadium', 'in', 'stanley', 'park', 'which', 'is', 'set', 'to', 'open', 'in', '2007', 'and', 'he', 'confessed', 'arsenal', '100m', 'deal', 'with', 'emirates', 'to', 'sponsor', 'their', 'new', 'ground', 'complete', 'with', 'naming', 'rights', 'has', 'given', 'the', 'anfield', 'club', 'serious', 'food', 'for', 'thought', 'he', 'said', 'have', 'to', 'say', 'historically', 'it', 'is', 'something', 'have', 'been', 'against', 'and', 'have', 'been', 'on', 'record', 'as', 'saying', 'that', 'but', 'think', 'the', 'size', 'of', 'the', 'arsenal', 'deal', 'is', 'real', 'eye', 'opener', 'would', 'say', 'in', 'the', 'past', 'deals', 'have', 'been', 'done', 'frankly', 'far', 'too', 'cheaply', 'and', 'it', 'just', 'hasn', 'even', 'been', 'worth', 'contemplating', 'but', 'the', 'arsenal', 'deal', 'is', 'the', 'sort', 'of', 'deal', 'that', 'causes', 'you', 'to', 'draw', 'breath', 'and', 'say', 'wow', 'that', 'interesting', 'my', 'personal', 'point', 'of', 'view', 'is', 'that', 'would', 'find', 'it', 'hell', 'of', 'lot', 'more', 'palatable', 'than', 'shared', 'stadium', 'some', 'liverpool', 'fans', 'would', 'find', 'such', 'move', 'highly', 'controversial', 'but', 'parry', 'countered', 'recognise', 'it', 'would', 'be', 'an', 'emotive', 'issue', 'for', 'many', 'supporters', 'but', 'you', 'look', 'at', 'the', 'amount', 'of', 'money', 'available', 'and', 'it', 'could', 'go', 'into', 'the', 'team', 'if', 'it', 'was', 'the', 'right', 'partner', 'how', 'strong', 'an', 'issue', 'is', 'it', 'time', 'will', 'tell', 'think', 'the', 'stadium', 'will', 'always', 'be', 'anfield', 'not', 'least', 'because', 'of', 'where', 'it', 'is', 'but', 'do', 'we', 'need', 'to', 'investigate', 'the', 'possibilities', 'of', 'sponsorship', 'think', 'it', 'would', 'be', 'remiss', 'not', 'to', 'that', 'not', 'to', 'say', 'we', 'have', 'made', 'decision', 'that', 'we', 'will', 'go', 'down', 'that', 'road', 'but', 'think', 'it', 'is', 'clearly', 'something', 'we', 'have', 'to', 'explore', 'on', 'the', 'subject', 'of', 'possible', 'new', 'investment', 'parry', 'revealed', 'liverpool', 'are', 'still', 'in', 'negotiations', 'with', 'mystery', 'investor', 'with', 'rumours', 'of', 'interest', 'from', 'the', 'middle', 'east', 'that', 'prompted', 'the', 'withdrawal', 'of', 'tycoon', 'steve', 'morgan', 'who', 'got', 'frustrated', 'by', 'failed', 'bids', 'and', 'what', 'he', 'claimed', 'was', 'indecision', 'by', 'the', 'board', 'he', 'also', 'accused', 'liverpool', 'of', 'using', 'him', 'as', 'stalking', 'horse', 'to', 'attract', 'other', 'bids', 'but', 'parry', 'explained', 'steve', 'has', 'never', 'been', 'used', 'as', 'stalking', 'horse', 'there', 'no', 'need', 'and', 'that', 'is', 'not', 'the', 'way', 'we', 'do', 'business', 'we', 'had', 'discussions', 'with', 'steve', 'over', 'the', 'course', 'of', '2004', 'think', 'we', 'came', 'close', 'to', 'concluding', 'deal', 'in', 'the', 'summer', 'but', 'it', 'didn', 'happen', 'quite', 'genuinely', 'the', 'new', 'interest', 'did', 'appear', 'relatively', 'late', 'in', 'the', 'day', 'just', 'prior', 'to', 'the', 'agm', 'in', 'december', 'and', 'as', 'have', 'said', 'it', 'was', 'of', 'such', 'potential', 'magnitude', 'and', 'that', 'potential', 'is', 'so', 'exciting', 'we', 'felt', 'we', 'had', 'to', 'evaluate', 'it', 'we', 'are', 'still', 'evaluating', 'it', 'steve', 'interest', 'was', 'taken', 'very', 'much', 'on', 'its', 'own', 'merits', 'his', 'enthusiasm', 'for', 'the', 'club', 'is', 'there', 'for', 'all', 'to', 'see', 'and', 'who', 'knows', 'what', 'the', 'next', 'few', 'months', 'will', 'hold', 'the', 'door', 'isn', 'closed', 'on', 'anything', 'we', 'had', 'perfectly', 'sensible', 'dialogue', 'with', 'steve', 'last', 'year', 'we', 'have', 'common', 'interest', 'in', 'making', 'liverpool', 'successful', 'that', 'dream', 'we', 'all', 'share', 'so', 'as', 'far', 'as', 'concerned', 'the', 'door', 'is', 'not', 'closed', 'would', 'take', '50m', 'if', 'we', 'had', 'no', 'investment', 'but', 'if', 'we', 'did', 'keep', 'him', 'as', 'for', 'the', 'stadium', 'if', 'it', 'gets', 'us', 'cash', 'what', 'difference', 'does', 'it', 'make', 'really', '50m', 'for', 'gerrard', 'don', 'care', 'who', 'you', 'are', 'the', 'directors', 'would', 'take', 'the', 'money', 'and', 'it', 'is', 'the', 'way', 'it', 'should', 'be', 'we', 'cannot', 'let', 'that', 'sum', 'of', 'money', 'go', 'despite', 'gerrard', 'quality', 'through', 'cleverly', 'worded', 'statement', 'the', 'club', 'has', 'effectively', 'forced', 'gerrard', 'to', 'publicly', 'make', 'the', 'decision', 'for', 'himself', 'which', 'think', 'is', 'the', 'right', 'thing', 'to', 'do', 'critical', 'time', 'for', 'liverpool', 'with', 'regards', 'to', 'gerrard', 'ideally', 'we', 'would', 'want', 'to', 'secure', 'his', 'future', 'to', 'the', 'club', 'for', 'the', 'long', 'term', 'am', 'hoping', 'he', 'doesn', 'walk', 'out', 'of', 'the', 'club', 'like', 'michael', 'owen', 'did', 'for', 'very', 'little', 'cash', '50m', 'realistically', 'would', 'allow', 'rafa', 'to', 'completely', 'rebuild', 'the', 'squad', 'however', 'if', 'we', 'can', 'afford', 'to', 'do', 'this', 'and', 'keep', 'gerrard', 'we', 'will', 'be', 'better', 'for', 'it', 'would', 'however', 'be', 'happy', 'with', 'gerrard', 'transfer', 'for', 'any', 'fee', 'over', '35m', 'parry', 'statements', 'are', 'clever', 'in', 'that', 'any', 'future', 'gerrard', 'transfer', 'cannot', 'be', 'construed', 'as', 'lack', 'of', 'ambition', 'by', 'the', 'club', 'to', 'not', 'try', 'and', 'keep', 'their', 'best', 'players', 'upping', 'the', 'ante', 'is', 'another', 'smart', 'move', 'by', 'parry', 'would', 'keep', 'gerrard', 'no', 'amount', 'of', 'money', 'could', 'replace', 'his', 'obvious', 'love', 'of', 'the', 'club', 'and', 'determination', 'to', 'succeed', 'the', 'key', 'is', 'if', 'gerrard', 'comes', 'out', 'and', 'says', 'that', 'he', 'is', 'happy', 'clearly', 'if', 'he', 'isn', 'then', 'we', 'would', 'be', 'foolish', 'not', 'to', 'sell', 'the', 'worrying', 'thing', 'is', 'who', 'would', 'you', 'buy', 'or', 'who', 'would', 'come', 'pending', 'possible', 'non', 'champions', 'league', 'football']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tokenize = CountVectorizer().build_tokenizer() # build tokenizer apply in CountVectorizer\n",
    "# convert to lowercase, then tokenize\n",
    "tokens1 = tokenize(doc1.lower())\n",
    "print(tokens1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We immediately see that many of the words here are not useful (e.g. \"to\", \"the\" etc.). Scikit-learn provides a list of such *stop words*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'throughout', 'off', 'no', 'ever', 'this', 'describe', 'became', 'few', 'same', 'other', 'forty', 'fire', 'something', 'perhaps', 'whoever', 'between', 'whither', 'within', 'still', 'over', 'please', 'an', 'wherever', 'empty', 'side', 'into', 'down', 'enough', 'twenty', 'may', 'latter', 'and', 'co', 'there', 'interest', 'much', 'than', 'toward', 'already', 'was', 'nor', 'ie', 'wherein', 'else', 'so', 'cant', 'whereupon', 'thick', 'fifteen', 'are', 'us', 'after', 'again', 'everything', 'upon', 'each', 'back', 'here', 'first', 'hasnt', 'since', 'him', 'hundred', 'eleven', 'always', 'in', 'amongst', 'for', 'besides', 'would', 'more', 'thereby', 'full', 'very', 'the', 'onto', 'ltd', 'their', 'is', 'one', 'yourself', 'up', 'de', 'least', 'many', 'elsewhere', 'through', 'who', 'although', 'sixty', 'whether', 'themselves', 'her', 'indeed', 'therein', 'get', 'put', 'namely', 'another', 'others', 'thus', 'among', 'anyway', 'couldnt', 'herein', 'before', 'could', 'once', 'during', 're', 'almost', 'himself', 'hereupon', 'most', 'nevertheless', 'whatever', 'last', 'can', 'she', 'we', 'two', 'mine', 'whose', 'several', 'therefore', 'ours', 'done', 'everyone', 'hereby', 'nine', 'thin', 'beforehand', 'its', 'via', 'why', 'though', 'without', 'being', 'been', 'afterwards', 'am', 'rather', 'because', 'third', 'from', 'it', 'against', 'whence', 'someone', 'per', 'fifty', 'also', 'find', 'inc', 'somehow', 'moreover', 'were', 'on', 'everywhere', 'made', 'even', 'what', 'five', 'give', 'take', 'me', 'thru', 'anyhow', 'beside', 'part', 'nothing', 'system', 'ourselves', 'cannot', 'be', 'sometimes', 'had', 'own', 'call', 'nobody', 'except', 'somewhere', 'as', 'formerly', 'bottom', 'i', 'yours', 'sincere', 'sometime', 'those', 'beyond', 'etc', 'keep', 'former', 'top', 'too', 'move', 'our', 'all', 'behind', 'might', 'hers', 'yourselves', 'hereafter', 'how', 'now', 'such', 'whereafter', 'go', 'becoming', 'across', 'out', 'both', 'four', 'mostly', 'thereafter', 'seem', 'twelve', 'until', 'either', 'seems', 'anyone', 'myself', 'at', 'latterly', 'only', 'however', 'should', 'show', 'detail', 'together', 'them', 'thereupon', 'con', 'his', 'to', 'must', 'they', 'but', 'eight', 'whole', 'nowhere', 'of', 'under', 'above', 'have', 'which', 'whenever', 'becomes', 'anywhere', 'eg', 'become', 'while', 'that', 'my', 'will', 'itself', 'he', 'amount', 'anything', 'neither', 'any', 'whom', 'mill', 'bill', 'never', 'fill', 'herself', 'a', 'seeming', 'seemed', 'has', 'some', 'found', 'front', 'name', 'un', 'thence', 'where', 'towards', 'noone', 'these', 'well', 'amoungst', 'yet', 'your', 'do', 'whereas', 'below', 'see', 'about', 'alone', 'less', 'otherwise', 'you', 'next', 'if', 'meanwhile', 'along', 'further', 'when', 'none', 'then', 'every', 'with', 'ten', 'whereby', 'due', 'cry', 'serious', 'not', 'hence', 'around', 'three', 'often', 'by', 'or', 'six'})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "stopwords = text.ENGLISH_STOP_WORDS\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter out these stopwords from our document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['parry', 'puts', 'gerrard', 'money', 'listen', 'interview', 'sport', 'bbc', 'sport', 'website', '1900', 'gmt', 'parry', 'speaking', 'exclusively', 'bbc', 'sport', 'admits', 'gerrard', 'constantly', 'linked', 'chelsea', 'final', 'say', 'future', 'told', 'bbc', 'live', 'steven', 'money', 'future', 'liverpool', 'doesn', 'matter', '30m', '40m', '50m', 'accept', 'offers', 'realistic', 'know', 'steven', 'subject', 'liverpool', 'finances', 'parry', 'revealed', 'club', 'ready', 'explore', 'possibility', 'sponsorship', 'deal', 'proposed', 'new', 'stadium', 'responding', 'criticism', 'bbc', 'sport', 'pundit', 'liverpool', 'stalwart', 'alan', 'hansen', 'insisted', 'talks', 'new', 'investment', 'ongoing', 'added', 'door', 'closed', 'shareholder', 'lifelong', 'fan', 'steve', 'morgan', 'parry', 'joined', 'liverpool', 'chief', 'executive', 'july', '1998', 'similar', 'role', 'premier', 'league', 'highs', 'lows', 'time', 'charge', 'anfield', 'busy', 'summer', 'overseeing', 'arrival', 'new', 'manager', 'rafael', 'benitez', 'managing', 'hold', 'steven', 'gerrard', 'subject', 'liverpool', 'captain', 'prize', 'asset', 'parry', 'revealed', 'real', 'madrid', 'did', 'ask', 'option', 'england', 'midfield', 'man', 'negotiations', 'striker', 'fernando', 'morientes', 'said', 'looking', 'ways', 'saying', 'got', 'deal', 'fernando', 'morientes', 'response', 'real', 'madrid', 'steven', 'sale', 'asked', 'gerrard', 'liverpool', 'player', 'day', 'season', 'parry', 'said', 'sincerely', 'hope', 'steven', 'knows', 'views', 'knows', 'rafa', 'views', 'affirmed', 'recently', 'steven', 'trying', 'build', 'team', 'crave', 'success', 'does', 'know', 'ambitious', 'argue', 'think', 'steven', 'dearly', 'love', 'win', 'things', 'liverpool', 'like', 'want', 'progress', 'season', 'lot', 'players', 'feel', 'common', 'aim', 'expected', 'chelsea', 'test', 'liverpool', '30m', 'plus', 'bid', 'summer', 'parry', 'claims', 'mood', 'listen', 'lot', 'open', 'secrets', 'steven', 'complete', 'myths', 'suggested', 'deal', 'tied', 'summer', 'didn', 'offer', 'summer', 'parry', 'explained', 'told', 'chelsea', 'far', 'concerned', 'sale', 'didn', 'want', 'sell', 'reality', 'didn', 'maybe', 'offer', 'summer', 'maybe', 'won', 'position', 'want', 'steven', 'stay', 'realistic', 'respect', 'steven', 'respect', 'know', 'decision', 'crucial', 'going', 'player', 'like', 'steven', 'just', 'doesn', 'work', 'idea', 'going', 'accept', 'offers', 'steven', 'tell', 'way', 've', 'decided', 'sell', 'agenda', 'forget', 'parry', 'currently', 'process', 'finalising', 'funding', 'liverpool', 'new', 'stadium', 'stanley', 'park', 'set', 'open', '2007', 'confessed', 'arsenal', '100m', 'deal', 'emirates', 'sponsor', 'new', 'ground', 'complete', 'naming', 'rights', 'given', 'anfield', 'club', 'food', 'thought', 'said', 'say', 'historically', 'record', 'saying', 'think', 'size', 'arsenal', 'deal', 'real', 'eye', 'opener', 'say', 'past', 'deals', 'frankly', 'far', 'cheaply', 'just', 'hasn', 'worth', 'contemplating', 'arsenal', 'deal', 'sort', 'deal', 'causes', 'draw', 'breath', 'say', 'wow', 'interesting', 'personal', 'point', 'view', 'hell', 'lot', 'palatable', 'shared', 'stadium', 'liverpool', 'fans', 'highly', 'controversial', 'parry', 'countered', 'recognise', 'emotive', 'issue', 'supporters', 'look', 'money', 'available', 'team', 'right', 'partner', 'strong', 'issue', 'time', 'tell', 'think', 'stadium', 'anfield', 'need', 'investigate', 'possibilities', 'sponsorship', 'think', 'remiss', 'say', 'decision', 'road', 'think', 'clearly', 'explore', 'subject', 'possible', 'new', 'investment', 'parry', 'revealed', 'liverpool', 'negotiations', 'mystery', 'investor', 'rumours', 'middle', 'east', 'prompted', 'withdrawal', 'tycoon', 'steve', 'morgan', 'got', 'frustrated', 'failed', 'bids', 'claimed', 'indecision', 'board', 'accused', 'liverpool', 'using', 'stalking', 'horse', 'attract', 'bids', 'parry', 'explained', 'steve', 'used', 'stalking', 'horse', 'need', 'way', 'business', 'discussions', 'steve', 'course', '2004', 'think', 'came', 'close', 'concluding', 'deal', 'summer', 'didn', 'happen', 'quite', 'genuinely', 'new', 'did', 'appear', 'relatively', 'late', 'day', 'just', 'prior', 'agm', 'december', 'said', 'potential', 'magnitude', 'potential', 'exciting', 'felt', 'evaluate', 'evaluating', 'steve', 'taken', 'merits', 'enthusiasm', 'club', 'knows', 'months', 'hold', 'door', 'isn', 'closed', 'perfectly', 'sensible', 'dialogue', 'steve', 'year', 'common', 'making', 'liverpool', 'successful', 'dream', 'share', 'far', 'concerned', 'door', 'closed', '50m', 'investment', 'did', 'stadium', 'gets', 'cash', 'difference', 'does', 'make', 'really', '50m', 'gerrard', 'don', 'care', 'directors', 'money', 'way', 'let', 'sum', 'money', 'despite', 'gerrard', 'quality', 'cleverly', 'worded', 'statement', 'club', 'effectively', 'forced', 'gerrard', 'publicly', 'make', 'decision', 'think', 'right', 'thing', 'critical', 'time', 'liverpool', 'regards', 'gerrard', 'ideally', 'want', 'secure', 'future', 'club', 'long', 'term', 'hoping', 'doesn', 'walk', 'club', 'like', 'michael', 'owen', 'did', 'little', 'cash', '50m', 'realistically', 'allow', 'rafa', 'completely', 'rebuild', 'squad', 'afford', 'gerrard', 'better', 'happy', 'gerrard', 'transfer', 'fee', '35m', 'parry', 'statements', 'clever', 'future', 'gerrard', 'transfer', 'construed', 'lack', 'ambition', 'club', 'try', 'best', 'players', 'upping', 'ante', 'smart', 'parry', 'gerrard', 'money', 'replace', 'obvious', 'love', 'club', 'determination', 'succeed', 'key', 'gerrard', 'comes', 'says', 'happy', 'clearly', 'isn', 'foolish', 'sell', 'worrying', 'thing', 'buy', 'come', 'pending', 'possible', 'non', 'champions', 'league', 'football']\n"
     ]
    }
   ],
   "source": [
    "filtered_tokens1 = []\n",
    "for token in tokens1:\n",
    "    if not token in stopwords:\n",
    "        filtered_tokens1.append(token)\n",
    "print(filtered_tokens1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will repeat this process for all documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 45 filtered token lists\n"
     ]
    }
   ],
   "source": [
    "all_filtered_tokens = []\n",
    "for doc in raw_documents:\n",
    "    # tokenize the next document\n",
    "    tokens = tokenize(doc.lower())\n",
    "    # remove the stopwords\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if not token in stopwords:\n",
    "            filtered_tokens.append(token)  \n",
    "    # add to the overall list\n",
    "    all_filtered_tokens.append( filtered_tokens )\n",
    "print(\"Created %d filtered token lists\" % len(all_filtered_tokens) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple type of analysis that we might do is to count the number of times specific terms (words) appear in our corpus. We could do this by creating a dictionary of term frequency counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3063 unique terms in this corpus\n"
     ]
    }
   ],
   "source": [
    "counts = {}\n",
    "# process filtered tokens for each document\n",
    "for doc_tokens in all_filtered_tokens:\n",
    "    for token in doc_tokens:\n",
    "        # increment existing?\n",
    "        if token in counts:\n",
    "            counts[token] += 1\n",
    "        # a new term?\n",
    "        else:\n",
    "            counts[token] = 1\n",
    "print(\"Found %d unique terms in this corpus\" % len(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to find the terms in the dictionary with the highest counts. Python provides a convenient way of doing this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted_counts = sorted(counts.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above creates a list of tuple pairs, where the first value is the key (i.e. the term) and the second value is the value (i.e the count). Let's display the top 20 terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yukos (count=118)\n",
      "said (count=106)\n",
      "oil (count=97)\n",
      "liverpool (count=83)\n",
      "gazprom (count=60)\n",
      "russian (count=56)\n",
      "win (count=50)\n",
      "deal (count=42)\n",
      "gerrard (count=41)\n",
      "chelsea (count=41)\n",
      "russia (count=40)\n",
      "league (count=39)\n",
      "auction (count=39)\n",
      "rosneft (count=37)\n",
      "company (count=37)\n",
      "firm (count=37)\n",
      "court (count=37)\n",
      "yugansk (count=35)\n",
      "bankruptcy (count=35)\n",
      "year (count=34)\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    term = sorted_counts[i][0]\n",
    "    count = sorted_counts[i][1]\n",
    "    print( \"%s (count=%d)\" % ( term, count )  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Bag-of-Words Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the *bag-of-words model*, each document is represented by a vector in a *m*-dimensional coordinate space, where *m* is number of unique terms across all documents. This set of terms is called the corpus *vocabulary*. Note that the positioning (context) of terms within the original document is lost in this model.\n",
    "\n",
    "Since each document can be represented as a term vector, we can stack these vectors to create a full *document-term matrix*. We can easily create this matrix from a list of document strings using Scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 3288)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(raw_documents)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process also build a vocabulary for the corpus, both in the form of a list and in the form of a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary has 3288 distinct terms\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "vocab = vectorizer.vocabulary_\n",
    "print(\"Vocabulary has %d distinct terms\" % len(terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display some sample terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3288"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bristol', 'british', 'broke', 'brokerage', 'brom', 'brookings', 'brought', 'bruce', 'brunswick', 'brutal', 'bryan', 'buenos', 'build', 'building', 'built', 'bulgaria', 'bulgarian', 'burgas', 'buried', 'burnley', 'burren', 'business', 'businesses', 'bust', 'busy', 'but', 'butler', 'buy', 'buyer', 'buying']\n"
     ]
    }
   ],
   "source": [
    "print(terms[500:530])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each column in the document-term matrix correspond to a term, we can look up the column associated with each term using the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1217"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what column is the term 'football' on?\n",
    "vocab[\"football\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3246"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what column is the term 'world' on?\n",
    "vocab[\"world\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the same Scikit-learn functionality to create a document-term matrix with N-grams. We specify an extra parameter ngram_range which specifies the shortest and longest token sequences to include. Length 1 is just a single token.\n",
    "\n",
    "For instance, transform our input documents into a matrix, extracting single tokens and bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range = (1,2))\n",
    "X = vectorizer.fit_transform(raw_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the vocabulary is much larger now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary has 16075 distinct terms\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "vocab = vectorizer.vocabulary_\n",
    "print(\"Vocabulary has %d distinct terms\" % len(terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display some sample terms. Note that we see a mix of single tokens and bigrams (i.e. phrases of length 2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['admitted victory', 'adriatic', 'adriatic coast', 'advanced', 'advanced position', 'advantage', 'advantage and', 'advantage of', 'adverts', 'adverts for']\n"
     ]
    }
   ],
   "source": [
    "print(terms[510:520])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A range of steps can be used to process text input files to reduce the number of terms used to represent the text and to improve the resulting bag-of-words model. These include:\n",
    "- Minimum term length: Exclude terms of length < 2. Scikit-learn does this by default.\n",
    "- Case conversion: Converting all terms to lowercase. Scikit-learn does this by default.\n",
    "- Stop-word filtering: Remove terms that appear on a pre-defined \"blacklist\" of terms that are highly frequent and do- not convey useful information.\n",
    "- Low frequency filtering: Remove terms that appear in very few documents.\n",
    "- Stemming: Reduce words to their stems (or base forms).\n",
    "\n",
    "Scikit-learn allows us to perform one or more of these steps by adapting the CountVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the built-in list of stop-words for a given language by just specifying the name of the language (lower-case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(raw_documents)\n",
    "# Are standard stopwords gone?\n",
    "\"and\" in vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Benitez deflects blame from Dudek Liverpool manager Rafael Benitez has refused to point the finger of blame at goalkeeper Jerzy Dudek after Portsmouth claimed a draw at Anfield. Dudek fumbled a cross before Lomana LuaLua headed home an injury-time equaliser, levelling after Steven Gerrard put Liverpool ahead. Benitez said: \"It was difficult for Jerzy. It was an unlucky moment. \"He was expecting a cross from Matthew Taylor and it ended up like a shot, so I don\\'t blame him for what happened.\" Benitez admitted it was a costly loss of two points by Liverpool, who followed up their derby defeat against Everton with a disappointing draw. He said: \"We had many opportunities but didn\\'t score and, in the end, a 1-0 lead was not enough. \"If you don\\'t have any chances you have to think of other things, but when you are creating so many chances as we are there is nothing you can say to the players. It was a pity. \"We lost two points, but we have one more point in the table. Now we have another difficult game against Newcastle and we have to recover quickly from that.\"\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_documents[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we could use our own custom stop-word list, which might be more appropriate for specific applications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_stop_words = [ \"and\", \"the\", \"game\" ] \n",
    "vectorizer = CountVectorizer(stop_words=custom_stop_words)\n",
    "X = vectorizer.fit_transform(raw_documents)\n",
    "# Are custom stopwords gone?\n",
    "\"game\" in vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove low frequency terms that appear in fewer than a specified number of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of terms in model is 3288\n"
     ]
    }
   ],
   "source": [
    "# how many terms did we have with the last approach?\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(raw_documents)\n",
    "print(\"Number of terms in model is %d\" % len(vectorizer.vocabulary_) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of terms in model is 473\n"
     ]
    }
   ],
   "source": [
    "# build another matrix, but filter terms appearing in less than 5 documents\n",
    "vectorizer = CountVectorizer(min_df = 5)\n",
    "X = vectorizer.fit_transform(raw_documents)\n",
    "print(\"Number of terms in model is %d\" % len(vectorizer.vocabulary_) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stem tokens to their base form, we need to use functionality from another third party library: **NLTK**. You may need to install this package manually, either through the Conda interface or via the Conda command line tool, using:\n",
    "    \n",
    "    conda install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test out the standard English stemming algorithm (called the Porter Stemmer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot\n",
      "fli\n",
      "deni\n",
      "sale\n",
      "sea\n",
      "comput\n",
      "comput\n"
     ]
    }
   ],
   "source": [
    "# import the standard English stemming algorithm\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "words = ['plotted', 'flies', 'denied', 'sales', 'seas', 'computing', 'computed']\n",
    "# try stemming each sample word\n",
    "stemmer = PorterStemmer()\n",
    "for w in words:\n",
    "    print( stemmer.stem(w) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use NLTK stemming with Scikit-learn, we need to create a custom tokenisation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\theka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# define the function\n",
    "nltk.download('wordnet')\n",
    "def stem_tokenizer(text):\n",
    "    # use the standard scikit-learn tokenizer first\n",
    "    standard_tokenizer = CountVectorizer().build_tokenizer()\n",
    "    tokens = standard_tokenizer(text)\n",
    "\n",
    "    # then use NLTK to perform stemming on each token\n",
    "    stemmer = PorterStemmer()\n",
    "    stems = []\n",
    "    for token in tokens:\n",
    "        stems.append( stemmer.stem(token) )\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use our custom tokenizer with the standard CountVectorizer approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alex', 'alexand', 'alexei', 'all', 'alleg', 'allen', 'allow', 'almost', 'alon', 'along', 'alongsid', 'alonso', 'alreadi', 'also', 'altern', 'although', 'alvalad', 'alway', 'am', 'ambit']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=stem_tokenizer)\n",
    "X = vectorizer.fit_transform(raw_documents)\n",
    "# display some sample terms\n",
    "terms = vectorizer.get_feature_names()\n",
    "print(terms[200:220])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform lemmatisation in the same way, using NLTK with Sckit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function\n",
    "def lemma_tokenizer(text):\n",
    "    # use the standard scikit-learn tokenizer first\n",
    "    standard_tokenizer = CountVectorizer().build_tokenizer()\n",
    "    tokens = standard_tokenizer(text)\n",
    "    # then use NLTK to perform lemmatisation on each token\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    lemma_tokens = []\n",
    "    for token in tokens:\n",
    "        lemma_tokens.append( lemmatizer.lemmatize(token) )\n",
    "    return lemma_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we can use our custom tokenizer with the standard CountVectorizer approach. The output terms are somewhat easier to intrepret than those produced by stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['parry', 'put', 'gerrard', 'above', 'money', 'listen', 'to', 'the', 'full', 'interview', 'on', 'sport', 'five', 'and', 'bbc', 'website', 'from', '1900', 'gmt', 'but', 'speaking', 'exclusively', 'also', 'admits', 'who', 'ha', 'been', 'constantly', 'linked', 'with', 'chelsea', 'will', 'have', 'final', 'say']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=lemma_tokenizer)\n",
    "X = vectorizer.fit_transform(raw_documents)\n",
    "# display some sample terms\n",
    "print(list(vectorizer.vocabulary_.keys())[0:35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put all of these steps together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 682)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theka\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=\"english\",min_df = 3,tokenizer=lemma_tokenizer)\n",
    "X = vectorizer.fit_transform(raw_documents)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gerrard', 'money', 'bbc', 'gmt', 'speaking', 'ha', 'linked', 'chelsea', 'final', 'say', 'future', 'told', 'live', 'steven', 'liverpool', 'doesn', 'matter', 'offer', 'know', 'finance', 'revealed', 'club', 'ready', 'deal', 'new', 'stadium', 'alan', 'insisted', 'talk', 'investment', 'added', 'closed', 'shareholder', 'fan', 'steve']\n"
     ]
    }
   ],
   "source": [
    "print(list(vectorizer.vocabulary_.keys())[0:35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as including/excluding terms, we can also modify or weight the frequency values themselves. We can improve the usefulness of the document-term matrix by giving more weight to the more \"important\" terms.\n",
    "\n",
    "The most common normalisation is *term frequency–inverse document frequency* (TF-IDF). In Scikit-learn, we can generate at TF-IDF weighted document-term matrix by using TfidfVectorizer() in place of CountVectorizer()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 130)\t0.3974597791748038\n",
      "  (0, 204)\t0.21755328169056634\n",
      "  (0, 30)\t0.13831731070303818\n",
      "  (0, 54)\t0.088438210466365\n",
      "  (0, 111)\t0.034579327675759546\n",
      "  (0, 261)\t0.1423631472946755\n",
      "  (0, 124)\t0.12709054744517082\n",
      "  (0, 301)\t0.05019683116727541\n",
      "  (0, 183)\t0.03819778147145201\n",
      "  (0, 283)\t0.34167155350722117\n",
      "  (0, 184)\t0.27655952066779993\n",
      "  (0, 167)\t0.088438210466365\n",
      "  (0, 59)\t0.2277810356714808\n",
      "  (0, 243)\t0.03819778147145201\n",
      "  (0, 77)\t0.21338170139457197\n",
      "  (0, 211)\t0.1927835160936817\n",
      "  (0, 158)\t0.0992935718534424\n",
      "  (0, 9)\t0.02586094969309587\n",
      "  (0, 282)\t0.21755328169056634\n",
      "  (0, 55)\t0.0305738291672926\n",
      "  (0, 101)\t0.03625888028176106\n",
      "  (0, 253)\t0.034579327675759546\n",
      "  (0, 177)\t0.04739884783718552\n",
      "  (0, 298)\t0.07529524675091312\n",
      "  (0, 19)\t0.0953179105838781\n",
      "  :\t:\n",
      "  (0, 78)\t0.034579327675759546\n",
      "  (0, 289)\t0.03819778147145201\n",
      "  (0, 326)\t0.023054258878629028\n",
      "  (0, 49)\t0.07639556294290402\n",
      "  (0, 195)\t0.06915865535151909\n",
      "  (0, 245)\t0.033097857284480794\n",
      "  (0, 87)\t0.0305738291672926\n",
      "  (0, 83)\t0.034579327675759546\n",
      "  (0, 241)\t0.03819778147145201\n",
      "  (0, 118)\t0.0284726294589351\n",
      "  (0, 185)\t0.029479403488788337\n",
      "  (0, 294)\t0.03625888028176106\n",
      "  (0, 201)\t0.03625888028176106\n",
      "  (0, 221)\t0.03819778147145201\n",
      "  (0, 182)\t0.029479403488788337\n",
      "  (0, 16)\t0.03625888028176106\n",
      "  (0, 37)\t0.03625888028176106\n",
      "  (0, 143)\t0.07251776056352212\n",
      "  (0, 170)\t0.03819778147145201\n",
      "  (0, 36)\t0.029479403488788337\n",
      "  (0, 165)\t0.03625888028176106\n",
      "  (0, 263)\t0.034579327675759546\n",
      "  (0, 45)\t0.034579327675759546\n",
      "  (0, 50)\t0.0305738291672926\n",
      "  (0, 117)\t0.029479403488788337\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# we can pass in the same preprocessing parameters\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\",min_df = 5)\n",
    "X = vectorizer.fit_transform(raw_documents)\n",
    "# display some sample weighted values\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Cosine similarity*: Most common approach for measuring similarity between two documents in a bag-of-words representation is to look at the cosine of the angle between their corresponding two term vectors. The motivation is that vectors for documents containing similar terms will point in the same direction in the m-dimensional vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's find the most similar document to the first document in our collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parry puts Gerrard 'above money' Listen to the full interview on Sport on Five and the BBC Sport website from 1900 GMT. But Parry, speaking exclusively to BBC Sport, also admits Gerrard, who has been constantly linked with Chelsea, will have the final say on his future. He told BBC Five Live: \"Steve\n"
     ]
    }
   ],
   "source": [
    "# First document - just display the start of it\n",
    "print(raw_documents[0][0:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Measure the cosine similarity between the first document vector and all of the others\n",
    "max_cos = 0\n",
    "best_row = 0\n",
    "for row in range(1,X.shape[0]):\n",
    "    cos = cosine_similarity( X[0], X[row] )\n",
    "    # best so far?\n",
    "    if cos > max_cos:\n",
    "        max_cos = cos\n",
    "        best_row = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar document was row 16: cosine similarity = 0.550\n"
     ]
    }
   ],
   "source": [
    "print(\"Most similar document was row %d: cosine similarity = %.3f\" % ( best_row, max_cos ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liverpool pledge to keep Gerrard Liverpool chief executive Rick Parry insists the club will never sell Steven Gerrard amid reports Chelsea will renew their bid to lure him from Anfield. Gerrard reiterated his desire to win trophies with the Reds after his superb Champions League winner on Wednesday.\n"
     ]
    }
   ],
   "source": [
    "# Best document - just display the start of it\n",
    "print(raw_documents[best_row][0:300])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
