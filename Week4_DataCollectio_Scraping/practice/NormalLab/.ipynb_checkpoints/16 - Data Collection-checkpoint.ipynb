{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection & Data Formats\n",
    "\n",
    "Term 1 2019 - Instructor: Teerapong Leelanupab\n",
    "\n",
    "Teaching Assistant: Suttida Satjasunsern\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Data\n",
    "The built-in Python *urllib.request* module has functions which help in downloading content from HTTP URLs using minimal code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tThe establishment of the Nondhaburi Telecommunication Training Center on August 24, 1960 with academic cooperation from the Government of Japan marked the origination of KMITL. The training center became the Nodhaburi Institute of Telecommunications under the Columbo Plan, later in 1964.\n",
      "\tAs specified by the 1971 King Mongkut's Institute of Technology Act, KMITL was originated by an amalgamation of three technical colleges: Nondhaburi Institute of Telecommunications, North Bangkok Technical College and Thonburi Technical College. In the same year, the Nondhaburi Institute of Telecommunications, or known as King Mongkut's Institute of Technology at Nondhaburi Campus, was relocated to the district of Ladkrabang in Bangkok. The new campus was called ''Chao Khun Taharn Ladkrabang Campus''. The Nondhaburi Institute of Telecommunications became the Faculty of Engineering in 1972. In the same year, the College of Design and Construction located at the Bangplad district was transformed into the Faculty of Architecture affiliated with KMITL.\n",
      "\tIn 1986, KMITL became a legitimate public university under a legislation called ''King Mongkut's Institute of Technology Ladkrabang Act'', which regulates its governance, administration and operation. Like other public universities in Thailand, KMITL is a government agency under the supervision of the former Ministry of University Affairs, which was merged with the Ministry of Education on July 7, 2003. At present, KMITL, like other public or autonomous universities, is under the supervision of the Commission on Higher Education affiliated with the Ministry of Education.\n",
      "    The name of the institute was derived from the name of King Rama IV. The royal grand crown seal has been graciously used as the emblem of the institute. The name \"Chao Khun Taharn\" has been used in honor of Chao Phya Surawong Waiyawat (Won Boonnak) of whom the heiress, Liam Prot Pitaya Payat, donated her own land to establish the institute. In 1977, the Faculty of Industrial Education and Science was established for offering fundamental courses for all faculties and for providing education and promoting research in science. In 1979, the Chao Khun Taharn Agricultural College, which was affiliated with the Department of Vocational Education, Ministry of Education, was transferred to KMITL, and became the Faculty of Agricultural Technology. The Computer Research and Service Center and the School of Graduate Studies were founded in 1981 and 1986, respectively. In 1988, the Faculty of Industrial Education and Science was separated into two faculties: the Faculty of Industrial Education and the Faculty of Science. In 1991, the Central Library was founded. In 1995, KMITL opened its first remote campus in Chumphon Province with 1,800 hectares of land in order to support the development of industry in the southern part of Thailand.\n",
      "    As information technology is of vital importance in our increasingly globalized and competitive world, in 1996, the Faculty of Information Technology was founded with the aim to provide higher education and research programs in information technology. In 2000, the department of Agricultural Industry within the Faculty of Agricultural Technology was endorsed to become the Faculty of Agroindustry. Moreover, there are several academical collaborations between King Mongkut's Institute of Technology Ladkrabang and the Japanese government since the signing of Memorandum of Understanding (MOU) in 1960\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = \"https://www.it.kmitl.ac.th/~teerapong/resources/ds4biz/week4/kmitl.txt\"\n",
    "response = urllib.request.urlopen(url)\n",
    "text = response.read().decode()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we may often want to wrap code to fetch URLs in a try block, to handle the case where we cannot access the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve https://somemissinglink.it.kmitl.ac.th/~teerapong/resources/ds4biz/week4/kmitl.txt\n"
     ]
    }
   ],
   "source": [
    "url = \"https://somemissinglink.it.kmitl.ac.th/~teerapong/resources/ds4biz/week4/kmitl.txt\"\n",
    "try:\n",
    "    response = urllib.request.urlopen(url)\n",
    "    text = response.read().decode()\n",
    "except:\n",
    "    print(\"Failed to retrieve %s\" % url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CSV (\"Comma Separated Values\") file format is often used to exchange tabular data between different applications, like Excel. Essentially a CSV file is a plain text file where values are split by a comma separator. Alternatively can be tab or space separated. \n",
    "\n",
    "We could download a CSV file using *urllib.request* and manually parse it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the CSV and store as a string\n",
    "url = \"https://www.it.kmitl.ac.th/~teerapong/resources/ds4biz/week4/goal_scorers.csv\"\n",
    "response = urllib.request.urlopen(url)\n",
    "raw_csv = response.read().decode()\n",
    "# Parse each line\n",
    "lines = raw_csv.split(\"\\n\")\n",
    "for l in lines:\n",
    "    l = l.strip()\n",
    "    print(l)\n",
    "    print(len(l))\n",
    "#     if len(l) > 0:\n",
    "#         # split based on a comma separator\n",
    "#         parts = l.split(\",\")\n",
    "#         print(parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can also use Pandas to directly download and parse CSV data for us, to create a Data Frame which is ready to analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"https://www.it.kmitl.ac.th/~teerapong/resources/ds4biz/week4/goal_scorers.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[JSON](http://json.org/) is a lightweight format which is becoming increasingly popular for online data exchanged. Based originally on the JavaScript language and (relatively) easy for humans to read and write\n",
    "\n",
    "The built-in module *json* provides an easy way to encode and decode data in JSON in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try downloading and parsing a simple JSON file which contains information about a number of books, originally from librarything.com:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.it.kmitl.ac.th/~teerapong/resources/ds4biz/week4/books.json\"\n",
    "response = urllib.request.urlopen(url)\n",
    "raw_json = response.read().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now parse the JSON, converting it from a string into a useful Python data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.loads(raw_json)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now iterate through the books in the list and extract the relevant information that we require."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book in data:\n",
    "    print( \"%s = %d\" % ( book[\"title\"], book[\"year\"] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use json_normalize in Pandas to create a Data Frame of semi-structured JSON data to make it ready to analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.json import json_normalize\n",
    "\n",
    "df = json_normalize(data)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR\n",
    "Alternatively, we can also use Pandas to directly download and parse JSON data for us, to create a Data Frame which is ready to analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "link = \"https://www.it.kmitl.ac.th/~teerapong/resources/ds4biz/week4/books.json\" \n",
    "df = pd.read_json( link, orient=\"records\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with XML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Extensible Markup Language (XML) is a markup language that defines a set of rules for encoding documents in a format which is both human-readable and machine-readable. XML is a widely-adopted format. Python includes several built-in modules for parsing XML data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *xml.etree.ElementTree* module can be used to extract data from a simple XML file based on its tree structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the content\n",
    "url = \"https://www.it.kmitl.ac.th/~teerapong/resources/ds4biz/week4/books.xml\"\n",
    "response = urllib.request.urlopen(url)\n",
    "raw_xml = response.read().decode()\n",
    "print(raw_xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the *xml.etree.ElementTree.fromstring()* function to parse content from a string containing XML data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as et\n",
    "xroot = et.fromstring(raw_xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An XML tree has a root node (i.e. the top level of the document), with child nodes at lower levels. We can iterate over these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for child in xroot:\n",
    "    # get the name of the tag, along with any XML attributes which the tag has\n",
    "    print( child.tag, child.attrib )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also query to find tags with specific names, such as '<book>' and then in turn find child nodes of that tag with a specific name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book in xroot.findall(\"book\"):\n",
    "    # get the text inside a <title> tag, contained within a <book> tag\n",
    "    title = book.find(\"title\").text\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can parse xml to Pandas dataframes, which is ready to analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cols = [\"id\", \"title\", \"ISBN\", \"year\", \"rating\", \"language\"]\n",
    "df = pd.DataFrame(columns = df_cols)\n",
    "\n",
    "for node in xroot: \n",
    "    s_id = node.attrib.get(\"id\")\n",
    "    s_title = node.find(\"title\").text\n",
    "    s_isbn = node.find(\"ISBN\").text\n",
    "    s_year = node.find(\"year\").text\n",
    "    s_rating = node.find(\"rating\").text\n",
    "    s_language = node.find(\"language\").text\n",
    "    \n",
    "    #print(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s \" % (s_id, s_title, s_isbn, s_year, s_rating, s_language))\n",
    "    df = df.append(pd.Series([s_id, s_title, s_isbn, s_year, s_rating, s_language], \n",
    "                                index = df_cols), \n",
    "                                ignore_index=True)\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[HyperText Markup Language (HTML)](https://en.wikipedia.org/wiki/HTML) is a language that web pages are created in. HTML isn’t a programming language, like Python — instead, it’s a markup language that tells a browser how to layout content. HTML allows you to do similar things to what you do in a word processor like Microsoft Word — make text bold, create paragraphs, and so on. Because HTML isn’t a programming language, it isn’t nearly as complex as Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The built-in Python urllib.request module has functions which help in downloading content from HTTP URLs using minimal code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "link = \"https://www.it.kmitl.ac.th/~teerapong/resources/ds4biz/week4/sample_web/sample.html\" \n",
    "response = urllib.request.urlopen(link)\n",
    "html = response.read().decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simple use the for-loop to read the html file line by line to see its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = html.strip().split(\"\\n\")\n",
    "for l in lines:\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The requests library\n",
    "\n",
    "The first thing we’ll need to do to scrape a web page is to download the page. We can download pages using the Python [requests](https://2.python-requests.org//en/master/) library. The requests library will make a `GET` request to a web server, which will download the HTML contents of a given web page for us. There are several different types of `requests` we can make using requests, of which `GET` is just one. If you want to learn more, check out this tutorial for using [API](https://www.dataquest.io/blog/python-api-tutorial/) requests in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try downloading a simple sample website, [https://www.it.kmitl.ac.th/~teerapong/resources/ds4biz/week4/sample_web/sample.html](https://www.it.kmitl.ac.th/~teerapong/resources/ds4biz/week4/sample_web/sample.html). We’ll need to first download it using the requests.get method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "page = requests.get(\"https://www.it.kmitl.ac.th/~teerapong/resources/ds4biz/week4/sample_web/sample.html\")\n",
    "page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running our request, we get a [Response](https://2.python-requests.org//en/master/user/quickstart/#response-content) object. This object has a `status_code` property, which indicates if the page was downloaded successfully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `status_code` of `200` means that the page downloaded successfully. We won’t fully dive into status codes here, but a status code starting with a `2` generally indicates success, and a code starting with a `4` or a `5` indicates an error.\n",
    "\n",
    "We can print out the HTML content of the page using the content property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing a page with BeautifulSoup\n",
    "\n",
    "As you can see above, we now have downloaded an HTML document.\n",
    "\n",
    "We can use the [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) library to parse this document, and extract the text from the `h3` tag. We first have to import the library, and create an instance of the `BeautifulSoup` class to parse our document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 \n",
    "soup = bs4.BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "for match in soup.find_all(\"h3\"):\n",
    "    text = match\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple example of using an Online API, we will retrieve JSON data from the Wikipedia web API. The Wikipedia page for 'KMITL' is [here](https://en.wikipedia.org/wiki/King_Mongkut%27s_Institute_of_Technology_Ladkrabang). We can retrieve this data in a cleaner JSON format from the Wikipedia API endpoint (https://en.wikipedia.org/w/api.php)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"King_Mongkut%27s_Institute_of_Technology_Ladkrabang\"\n",
    "url = \"https://en.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&exintro=true&titles=\" + title\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = urllib.request.urlopen(url)\n",
    "raw_json = response.read().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have downloaded the JSON data into a string, we parse it using the *loads()* function, which will convert it into an actual Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.loads(raw_json)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response still needs to be inspected. Note that the results we want are are in *data[\"query\"][\"pages\"]*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"query\"][\"pages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data[\"query\"][\"pages\"][\"1232312\"]\n",
    "print(result[\"title\"])\n",
    "print(result[\"extract\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - Currency Exchange Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next example, we will use the *Fixer.io* API to get currency exchange rate information: http://fixer.io\n",
    "\n",
    "For API documentation: https://fixer.io/documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve all rates in EUROs, we retrieve the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_KEY = \"0c9904dea3d2c46b78686bc16bbba722\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://data.fixer.io/api/latest?access_key=\" + ACCESS_KEY\n",
    "response = urllib.request.urlopen(url)\n",
    "raw_json = response.read().decode(\"utf-8\")\n",
    "print(raw_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.loads(raw_json)\n",
    "# List all the rates\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a specific rate\n",
    "data[\"rates\"][\"CHF\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the URL to get rates for a different currency, such as US Dollars (USD):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://data.fixer.io/api/latest?access_key=\" + ACCESS_KEY + \"&symbols=USD\"\n",
    "print(url)\n",
    "# Retrieve the JSON\n",
    "response = urllib.request.urlopen(url)\n",
    "raw_json = response.read().decode(\"utf-8\")\n",
    "# Parse the JSON\n",
    "data = json.loads(raw_json)\n",
    "# Display the rates data for US dollars\n",
    "data[\"rates\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = json_normalize(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
