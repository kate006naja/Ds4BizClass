{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalization, Fitting Models and Cross Validation\n",
    "\n",
    "\n",
    "Term 1 2019 - Instructor: Teerapong Leelanupab\n",
    "\n",
    "Teaching Assistant: Suttida Satjasunsern\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we will be using\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='ticks', palette='Set2')\n",
    "\n",
    "# some custom libraries!\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from ds_utils.decision_surface import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we're importing library code that we've developed just for this class. In the future, new common code will continue to be added to the `ds_utils` folder. Have some code you use frequently? Consider adding it to that folder as your own library!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivational example\n",
    "\n",
    "Imagine we have some noisy observations from a nonlinear function. We're going to approximate that function by fitting a polynomial to the observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEPCAYAAABoekJnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl8VOXZ+P/PzGQl+74QlizkBmTfBXEDUWjVWlxRrPaLgq1WWrXt0youFav+tNqiFlwqWllaxaU+BR4KooJA2UGE3JBAgJAQEkgChOwzvz8m4DAkZJl95nq/XrzMnHPmnCsn47nm3g0WiwUhhBCis4yeDkAIIYRvk0QihBDCIZJIhBBCOEQSiRBCCIdIIhFCCOEQSSRCCCEcIolECCGEQySRCCGEcEiQJy6qlHoSuLX55b+11r+22z8IeBuIBr4GZmitG90bpRBCiPZwe4lEKTUemAAMBgYBQ5VSN9kd9gHwoNY6FzAA97k3SiGEEO3liaqtEuARrXW91roB2AN0P7tTKdUDCNdab2jeNB+4xe1RCiGEaBe3V21prb87+7NSqhfWKq4xNoekY002Z5UAGfbnUUrFArF2m0OALGAf0OSkkIUQwt+ZgDRgk9a6rqNv9kgbCYBS6hLg38BjWut9NruMgO1MkgbA3MIpZgJPui5CIYQIOGOBtR19k6ca28cAS4CZWuvFdruLsGbGs1KB4hZO8yrWai9bPYAvFyxYQGpqqpOiFUII/3b06FHuvPNOOL82qN3cnkiUUt2AT4HbtNZf2O/XWh9UStUqpcZorb8BpgLLWjiuEqi0OzcAqampZGRcUBsmhBDi4jrVJOCJEsmjQBjwp7MPfmAucAMwS2u9GbgTeEspFQ1sBf7igTiFEEK0gyca2x8GHm5h11ybY3YAI9wWlBBCiE7zWGO7EMJ/mc1mioqKqK6u9nQowk5ERAQZGRkYjc4b/SGJRAjhdOXl5RgMBpRSTn1gCceYzWaOHDlCeXk5ycnJTjuv/IWFEE5XWVlJSkqKJBEvYzQaSUlJoaqqyrnnderZhBACaGpqIjg42NNhiBYEBwfT2OjcqQslkQghXMJgMHg6BNECV/xdpI1ECOHXnn76abZu3UpDQwOHDh0iOzsbgLvvvpvJkyc77TqnT5/m7rvvpr6+njlz5pCZmenwOVevXk1hYSH33nsvixYtAuCOO+5w+LzOJolECOHXnnzSOpNSUVERd999N5999plLrrNnzx5CQkL4+OOPnXbOXbt2nfvZGxPIWZJIhBABa86cOWzfvp2SkhLuuusuli1bxoMPPsjIkSPPJZ4vvviC8vJyZs2axdGjRzEYDDzyyCOMHj363HmOHz/O7373O8rLy5kxYwYTJkxg48aNPP/88wBMnTqVBx98EIB58+YRFhZGQUEBSileeuklQkJCmD9/PosWLcJkMnHVVVdx0003sXixdQap9PR0ioutM0U99NBDrF69mldffRWz2Uy3bt145plnSExM5Oqrr+aGG25g7dq11NTU8MILL9CvXz+X30dJJEIIl1pfup91pftdcu7RKVlcmpLl0Dnq6+tZunQpAMuWXTAbEwCzZ89m8uTJjBs3jmPHjjFlyhQ+/fRTIiMjAUhISODZZ5/ltddeY+7cuRctlWzbto1ly5aRnJzMrbfeytq1a0lMTGThwoUsWbKE8PBwpk2bxsSJE7n99tsBmDx5MnPmzAGsSWvWrFksWrSIjIwM3n77bZ555hn+8hfrBCCxsbF89NFH/P3vf2fevHnn3udKkkiEEAFtwIABbR6zbt069u/ff+5h3djYyOHDh+nTp0+Hr9erV69zk8pmZ2dTVVXFgQMHuOqqq4iKigJg/vz5gLWNxN7OnTsZMGDAufkEb7vtNt58881z+8eOHXvuOitWrOhwfJ0hiUQI4VKXOqHU4EphYWHnvbZYrKtY2HaRNZvNvPfee8TGWpdAOnbsGAkJCa2e02AwnDsPQENDw7mfQ0NDLzguKCjovN5UpaWlhIeHt3hus/n8VTUsFst5sZ49vzt7zUn3XyGEaBYXF0d+fj4AK1euPLd91KhRLFy4EID8/Hyuv/56ampqLnqegoICLBYLhw8fRmt90esOGzaMr776iurqahobG3nkkUfYtWsXJpPpgjEfAwcOZMeOHRQVFQHwj3/8g5EjR3bq93UWKZEIIUSzadOm8dvf/pYlS5Ywbty4c9sff/xxZs2axfXXXw/Aiy++eK59pCWjR49myZIlXHfddWRmZjJ06NCLXveSSy7hrrvu4vbbb8dsNnPNNdcwevRogoOD+c1vfkNiYuK5YxMTE3nmmWd48MEHaWhoID09ndmzZzv4mzvGYFv88nVKqZ7AgVWrVsl6JEJ40J49ezrVfiDcw/7vU1RUdDZxZmqtCzt6PqnaEkII4RBJJEIIIRwiiUSIVpSVwaZN1v8KIVoniUSIFixabCY7t5HJ95wiO7eRRYv9py1RCGfzWK+t5vXY1wE/tG/cUUo9CfwUqGje9JbW+nX3RigCVVkZTH/AzIRXVpCQXcnxglimPzCB8eOCSErydHRCeB+PJBKl1EjgLSC3lUOGAbdrrde7LyohrAoLITa9hoTsSgASsiuJSauhsDBKEokQLfBU1dZ9wM+B4lb2DwN+p5TaqZR6TSkV1spxwgf4WltDz55QWRzO8QLrKObjBbFUlYTTs6dHwxLCa3mkRKK1ngaglLpgn1IqEtgGPAbkA/OBJ4Df2x0XC8TavV0Gj3iZRYvNTH/ATGx6DZXF4cz7q4k7bvfuBY+SkmDeX01Mf2ACMWk1VJVY45bSiO9avnw5b775Jo2NjVgsFm688UamTZvmkmt9/PHH5838Gwi8bmS71vo0MOnsa6XUy8DfsEskwEzgSTeGJjrIl9sa7rjdwPhxQRQWRtGzJ14fr78oK7NWLTrznpeWlvLCCy/w8ccfExcXR3V1NVOnTiUzM/O80eui87wukSilugPjtdZ/a95kABpaOPRVrKUVWxnAGtdFJzrC19sakpIkgbiTq0qvFRUVNDQ0UFtbC0BERATPP/88oaGhLFu2jHfffZfa2lrq6+t57rnnGDJkCFOnTqVv375s2bKFuro6Hn30Ud5//30KCgq45557uOeee5gzZw7FxcUUFBRQUVHBbbfddkEpZ+fOnfzxj3+ktraWuLg4nn76abp168a7777LJ598gtFoZMCAATzzzDMO/56e5HWJBKgBXlRKrQYKsbalfGJ/kNa6Eqi03dZSVZnwHNu2hrMlEmlrEC1xZem1d+/ejBs3jvHjx9OnTx9GjhzJ9ddfT7du3Zg1axZz584lPj6ejz76iDfffJO5c+cC1ll1P/roI1577TWeffZZ/vWvf3HixAl+9KMfcc899wDWFQwXL16M2Wzmxz/+MZdeeum569bX1/P4448zd+5c0tPTWbNmDU888QTvvPMO8+bNY82aNZhMJn7/+99TWlpKSkqKY7+oB3lNIlFKLQVmaa03K6WmA58DIcBa4GWPBic6RdoaRHu5uvT69NNP87Of/Yy1a9eydu1abr31Vl566SVef/11vvjiCw4cOMDGjRsxGr/vf3T55ZcD1tUJBw4cSHh4OF27duXkyZPnjvnhD39IREQEAFdffTUbNmwgLi6u+Xcq5PDhwzzwwAPnjj99+jQmk4nBgwdz8803M27cOO69916fTiLg4USite5p8/Mkm5+XAEs8EZNwLmlrEO3hytLrl19+yZkzZ5g0aRKTJ09m8uTJ/POf/2TBggX86U9/4oYbbmD48OEopViwYMG59wUHB5/7OSio5UelyWQ697PZbL7gdUZGxrk14puamigvLwfgjTfeYPv27Xz99ddMmzaNl156iREjRjj+y3qIjGwXLpeUBMOHSxIRrTtbel3xywksnX49K345wWml17CwMF5++eVz63dYLBb27NlDSEgIBoOBGTNmMHLkSP7zn//Q1NTUoXOvXLmS+vp6qqqqWL16NZdddtm5fVlZWVRVVbF582YAlixZwqOPPsqJEyeYNGkSubm5PPzww4wZM6bN9Uq8nddUbQkhApurSq+jRo3iwQcfZMaMGedWKhw7diyvv/46v/3tb5k4cSIGg4HLLruMLVu2dOjcoaGhTJkyhdOnTzN9+nRycnLYuXMnACEhIfz5z39m9uzZ1NXVERkZyQsvvEB8fDy33XYbN998M+Hh4WRmZjJ58mTn/LIeIuuR+ClXdKMUor0CYT2SOXPmAPDQQw95OJKOk/VIRJtam3DQ10aYe4K33iNvjUsIkKotv9NaN8qTJ4089hvfGmHubt46Ct9b4wp0vlgScRVJJH6mpW6UkUm1PPJYGNf92fdGmLuLt47C99a42sNisWAwSMLzNq5ozpCqLT/T8oSDYcS12Effc3F6m9bHMXg0LK+Nqy0mk+lcw7bwLg0NDa12Z+4sSSR+pqVulH962UTVUZnN9mJcOeOvI+0bvjoTcWxsLKWlpZjNZk+HImyYzWZKS0uJiYlx6nmlassPtdSNMjpaRphfjKtG4TvavuGrswMkJiZSVFTk8+Mj/FFERASJiYlOPad0/w0g0iW4bc68R2VlkJ3beF77xopfTqBgb8fbN+RvJ1zJ0e6/UiIJIDKbbduceY+cOX+U/O2EN5M2EuF0MubBylfbN4ToKCmReAl/qbqQMQ/f89X2DSE6ShKJF/CXh68vj3lwFZn9WAQCSSQe5k8PX19fEdFVvKl9w19KvsK7SBuJhzljwJm3tElIm4B3a20ONiEcJSUSD3N0QR9vqhaTNgHv5U8lX+F9JJF4mCMPX298OEibgHeSakfhSpJIvEBnH77e+nDwpjYBYeXKpWyF8FgiUUpFA+uAH9qPpFRKDQLeBqKBr4EZWutGtwfpRp15+MrDQbSXVDsKV/JIIlFKjQTeAnJbOeQDYJrWeoNS6h3gPuCv7orPV8jDQXSEVDsKV/FUieQ+4OfA3+13KKV6AOFa6w3Nm+YDT2OXSJRSsUCs3dsDboItX3k41Dc1crKhlpP1tZxuqKO2qYGapgZqGxtotDTRZLbQhBksYDIYMRkNmAwmwkxBhAUFE24KJjI4lOjgMKJCwggzBXv6V/JJUu0oXMEjiURrPQ1AKdXS7nSgxOZ1CS0niJnAk04Pzgd5y8OhuqGOkjMnKTlTxdGak5TXnuZ4bTXH605zprHttSmMBgMGDDRZ2p56PMwUREJoJIlhESSERZIaHk1al2jSusQQFRLmjF9HCNFO3tjYbgRsO7gbgJaeLK9iLa3YygDWuCYsYau6oZ79p8o4eOoEh06f4NDpCirqz5zbH2w0kRgWSUJoBFnRicSFdiE6OIzokDCigsMINwUTFhRMmCmYIKMRI4Zzq+lZLBbMWGgym8+VXGoaGzjdUMephlqq6muprD/D8dpqymuryasspc78fRNadHAY3SPj6B4ZT4/IeLKik4iW5CKEy3hjIikC0mxepwLF9gdprSuBStttrZRwhBOcbqglr7IUXVlK/skyis9UAdYsnxweTa+YJDIi40jvEkNalxjiQyMwdnKZVYPBgAkDJpOREFMQ0YRf9HiLxUJF/RmOnjlJ8Zkqik5XcPD0Cb6rOIql+TtJSngU2dFJqJgUesemEBvapVOxCSEu5HWJRGt9UClVq5Qao7X+BpgKLPN0XIHGbDFz4NRxdp44wu6KEg6frsCCtUopKzqJYUk9yIlOokdkPGFB7W+vcMUUHQaDgfjQCOJDI+gb9/13kPqmRg6drqDgZBn5J8vYcbyIdaX7AUjvEkOfuFQGxmeQE52EySiTPAjRWV6TSJRSS4FZWuvNwJ3AW81dhLcCf/FocF7GVfMlNZqb2FN5lK3lh/n2xBFONdRhxEBWdCLX9+hPn9g0ekTFYzJ07qHr7lH4IaYgcmKSyIlJ4lrAbLFQVF3Bnoqj7Kk8ylfF+1h1RNMlKJhL4tIZktidfnFphJi85n8LIXyCrJDoY5z9MDZbzORVlrKx7CA7jh/mTGMD4aZg+sWnMyC+K5fEpRMRHOJw3M5cLdBZahsb2F15lJ0njvDt8SOcbqwj1BjEgISuDEvqQb+4NIKMJs8EJ4QbyQqJfs629AHOmxKluLqS9ccOsPFYIZX1NYSZghmUkMGwpO70jk0l2MkPUG8chR8WFMyQxG4MSexGk8XM3spjbCk/xLbyw2wqO0hEUCjDk3pwaUomPSLjz3UGcCaZjVf4A0kkXsy+9PHrR00OPYzrmxrZUn6Ir0vy2X+qHKPBQL+4dG5NzmRAQlenJY+WHo7ePgrfZDDSJy6VPnGp3JE9jN2VJawvPcDao/l8WbKXbhFxjE3NYURyD8KDHC+hgXdNuCmEIySReKmWJmR8/uEJGAwdfxiX1Zxidcle1h3dT01TAynh0dycOZiRyZlO7xbb2sPRl0bhm4xG+sd3pX98V8401rOp7CBrSvJZWLCJD/dvJduYyTXdc+mXYT8etv1a+vveN30CCfFBDB4spRPhWySReKmWqoLiutYw485IXvhl2w9ji8VCXmUpq4rz2HWiGIPBwNDE7lye1ote0Ukuq6a5WNWbr4zCt9UlKIQr0npxeWoOb358nA937KP2iv3kHcgndk8KU4Yo+sd37XBXZ/u/b2VhNHX1cO9Dpzh1TEonwrdIIvFSrVUF3X+/gfvvb/1h3GQxs7XsECuO7OHQ6QqigkOZ1K0fl6fluHzsRHvaQbxlFH5HlZcbeGxaLBNeqcAUYqb6eBjV5lO8sftrUsOjuSajDyOTe7a7etD279sloYY1Lw/nxr96z3IAQnSEJBIv1VZVkP0DpsHcxPrS/fxf0W7Ka6tJCY9maq8RjEzOdHrDeWtaS36RkdYVHH2lFNIS+yQZkVDLsgd+wItvV5Bv3MPf9/2Xzwp3MCGjD5en9SK0jS7Etn/f8Ng6wmLqvaojghAdIYnEi7WnKqjB3MSaknxWFO2hov4MmVEJ3JI1lAGdqG5xVEvJ7yd3Gxk5utHnG5RbSpKVRyIYlx3DbYk9yKssZXnRd3x0YBvLD+9mfEZvrkzLJfwigzXP/n23bQti8q2NXtsRQYi2yDgSH9VobuKbo/tZengXlfU15EQn8YPu/egTm+qS9o+OONtrKzISRo72rrEjjli02ML0B5rOKyHaJ8WCk2UsPbSLXRUlRASFcG1GX65Mz22zhNKecwvhKjKOJMCYLWbWlx7g34d2cbyumpzoJH6qRqNiUzwd2jln20E2bfK+sSOOaE8JMTs6iYf6XUXhqeN8fnAnHxduZ+WRPCZ2u4SxaTmtVjN6oiOCjGERziKJxEdYLBZ2njjCp4U7KD5TRY/IeO7sNZy+sWkeL4G0xtvHjnRGezsL9IxK4KF+V5FfVcZnB3fwj/1bWFWcx409BjIsqUeL1Y7u7IggY1iEM0ki8QGFp47z0f5t7Dt5jOTwKO7vfRlDErt5bQI5y5fGjrhKTkwSv+o/jt2VJXx8YDvv6HWsKNrDLVlDPFaKbKubthAdJYnEi1XUneHTwu1sOFZIVHAYU7KHc1lqtk/NVOuLY0eczWAwcElcOn1i09hUVshnhTv507erGJiQweTMQaSER7s1Hm+crkb4NkkkXqi+qZEVRXtYXrQbi8XCdRl9ua7bJRftAeTNfHXsiLMZDQZGJmcyOKEbq4o1yw5/x9NblnJVei4/7N7fbX9ff6xyFJ4licSLWCwWdhwv4p/7t3K8rpqhid35ceYgEsMiPR2acKIQUxATu13C6JQsPivcyaojeWw8VsiPMwcxMjnT5d22pcpROJskEi9RWnOSxQVb2F1RQnqXGH7Vf5xX9cQSzhcTEs7duSO5PC2HxQWbmb93A1+X5DMlZzjdIuNcem2pchTOJInEwxrMTSw//B3LD+8myGji1qwhXJmW2+52EOnC6ft6RiXw64ET2HDsAB8f2Mbsbcu5Oj2X63sMcGl1l1Q5CmeRROJBuytKWJi/ibLa0wxP6sEtWUOICbn4+uS2pAun/zAaDIxOyWJgfAafFm7ni2LNlvJD3Jo11Cd66InAJonEA07V1/LRga1sOFZIcngUM/tdTZ+41A6dw91dOKXk4x4RwSHc2WsEo1OzWLBvE2/mrWVgfFduzxlGfGiEp8MTokUeSSRKqSnA40Aw8KrW+nW7/U8CPwUqmje9ZX+ML7JYLPz3WCH/3L+VmqZ6JnW7hEnd+3VqUkV3duGUko/7ZUYl8j+Dr+WLI5p/HdzJU1v+zU09B3JFWq7b51AToi1uTyRKqa7AbGAoUAesU0qt1lrvtjlsGHC71nq9u+NzlRN11SzYt5FdFSVkRSVyV68RdI3o/MJI7urCKYPXPMdkMHJNRh8GJ3ZjQf4mFhdsYXPZIab2GklqF/eOPRHiYjxRIhkPfKG1PgGglPoIuBl4xuaYYcDvlFI9gK+BR7XWtbYnUUrFAvZP4lZnavRU1YzFYmHN0XyWHNiG2WLhtqyhXJneC6PBsUGF7urCKYPXPC8xLJJfXHIlG44d4J/7t/KHrUu5vscArsnojcnBz5EQzuCJRJIOlNi8LgFGnH2hlIoEtgGPAfnAfOAJ4Pd255kJPNmeC3qqauZ4bTXv79tAXmUpvWNTuCtnJEnhzhsT4o4unDJ4zTsYDAYuTcmib1wai/I380nhdraVH+IedSlpXWI8HZ4IcJ5IJEbAdu56A2A++0JrfRqYdPa1Uupl4G9cmEhexZpkbGUAa2w3eKJqxmKx8E1pAR/u34oFuDNnBGNTs13S88bVXTi9dfBaoDb+x4SEM6PvWDaXHWRh/mae3bqMG3oO4JquvR0u5QrRWZ5IJEXAWJvXqUDx2RdKqe7AeK3135o3GYAG+5NorSuBStttSqkLLubuqpnKujP8fd9/2VVRgopJ4e7ckS4fme7qh6q3DV6Txn8YltSD3JhkFuRv4uMD29lxvIh7ci8lOTzK06GJAOSJRLISeEoplQRUA5OB+2321wAvKqVWA4XAz4FPOnsxd1bNbCo7yML8TTSYm5rbQlzfw8ZdD1VvGbwmjf/fiw4JZ0afsWwsK2RxgbV0cnPWYMam5si4E+FWbk8kWusjSqnfA6uBEOBtrfVGpdRSYJbWerNSajrwefP+tcDLnb2eO6pmqhvqWVSwiU1lB+kZlcC9uZe6pVdNID5UWyphRibVsnRpJJMmeUeycydD80SQuTEpvLd3AwvyN7HjeBF3547q0OBWIRzhkXEkWuuFwEK7bZNsfl4CLHHW9VxZNaMrS3l373qq6mu4oUd/rut2idt60gRijyr7Eub2hb0pOxTG48+f4qGZgVnNBRAX2oVf9LuKr0r2seTANp7espSpvUYwOLGbp0MTASBgRrY7u2qmwdzEvw7u5D9Fe0gOj+I3AyfQMyrBeRdoh0DsUWVbwoxMqqXsUBg3zg2cEtnFGA0GrkrPpXdsCn/T65i7Zw1jUrK5NXsIYSbfXIJA+IaASSTOdPRMFW/nreNwdQWXp+Zwc9YQQk3uv5Xe2qPK1c6WMJcujeTx50/5RYnMmR0m0rrE8JuBE/j80Lf83+Hd7Ksq5ccpY2gsTfCKzhLC/0h/wQ6wWCx8XZLPs9uWc6LuDD/rezl39hrRoSRSVgabNln/6wx33G6gYG8QH78XRcHeoICp1klKgkmToOqotUQG+GyJbNFiM9m5jUy+5xTZuY0sWmxp+01tCDKauKnnIH7VfxxVp5t4o2AFjy3aSk7v+vPO7+zPowhMUiJpp+qGOv6+byPbjh+mT2wq9+SOIja0S4fO4aoeVt7So8rd/KFE5uoOE3ENKSyYei3Xv/Vv1E15ZF2Xz8wHJjJ+XBQrV0k3auEckkjaYV/VMd7R66iqr+HHmYO4pmufDnfrDcQeVu7gbWNcOsrVHSYKCyEiponQSOtQrODwRia9tpylO0bz0AMp8nkUThHQVVttFevNFjP/e/BbXt65iiCDkd8MnMC1GX07NTak9QdG5+MXVklJMHy47yUROL/DBDi/es7+/BWHoqg+FsG60K+4dOYm+TwKpwjYEklb1UyVdWf4m16PriplRFJPpuQMd2i1ukDsYSXa5urquZbO/8ZfoSFuO1y9l8Y6E0GhTfJ5FA4JyETSVjXTrhPFzN+7nrqmRn6SO4pLkzMdHinsD/X5wjVcXT3X8vmHcfrTFLaE/BeL2YD+pLd8HkWnBWQiaa2aaf+BCL4+vYMVRXvo2iWW+waMcerMqr5eny9cxx2Tb9qf/6EfdWNfcTzv7/+G4Ec30JhaRn3TUEI80JVd+LaA/MS0VM3U0GBmpXElh4rKuTw1h1uyhrjkf6hA7WElvFOv9AieShvPvw7uZPnh3ew/Wc70PpeRKlPTiw4IyERiX80U0bWCyW//h9I6M1cFj2ZcbE9COr76rRA+yWQwclPPQeTGJPO3vPXM3racO3NGMCol09OhCR8RsL227rjdwF5t5Ddv53PFrLWENXbho/uu4df/L8Fpg8KE8CWXxKXzxJCJ9IxK4N2963l/73+pb2r0dFjCBwRkiQSgou4M75d8Q0FTGSNie/HIuAGMf3HVeY3vgwYGcfp04C2eJAJXbGgXZva/ms8Pfsuyw99ReOo49/e5zO/XiA/UhdKcJSBLJN9VFPPs1mUUVVcwTY1mQO1wopPqz2t8D46oZ/go505bIYQvMBmM/KjnQB665Eoq62t4bvtyNh0r9HRYLuOKKWoCTUCVSMwW87lvWuldYs990yoznN/4XrwtmZPlIZ2aVVa+2Qh/0S/eWtX1Vt43vK3Xse9kGbdkDSHY6D8NiDLjhHMETCI5WV/D23nr0FWljEnJ4vbsYed6Zdk3vp84HE5it45PWyFLwAp/ExfahUf6j+OTwh3858geDpw6zvQ+l7l8+Wh3CcQ1fVwhIBLJ3qpjvLVnLTVNDfwkdxSjU7IuOMZ2jEdkJIwc3bFR6PLNRvgrk9HIzVmDyYlJ4r2965m9bRn35F7KwIQMT4fmMJlxwjn8OpGYLRZWFO3hs8IdJIVHMrP/1XSNiG31eNsxHh0dhS7fbIS/G5SQQcbgiczbs5Y3dn/NhIw+/KjnQLetCOoKMuOEc3gkkSilpgCPA8HAq1rr1+32DwLeBqKBr4EZWusO9UOsbqhn/t717DxxhKGJ3Znaa2SH5sr8jjU/AAAW0klEQVTq6Ch0+WYjAkFiWCS/HngNH+7fyoqiPew/Wc59vcd0eEkFbyIzTjjO7YlEKdUVmA0MBeqAdUqp1Vrr3TaHfQBM01pvUEq9A9wH/LW91yiqruSNbVuorK/htqyhXJWe26m5sjoyCl2+2YhAEWw0MSVnONnRiXywbyPPblvOtN6j6R2b6unQOk1mnHBMm2VSpdQSpdR4J15zPPCF1vqE1roa+Ai42eZ6PYBwrfWG5k3zgVs6coE396zBjIXHBozn6q7K4QkX2ytQVysUgWlkcib/M+g6IoNCePXb1Sw9tAuzRbrOBqL2lEg+Bp5QSr0BvAn8TWt9woFrpgMlNq9LgBFt7L+gVU8pFQvYN3hkAGRFJzJz8EQig0MdCLNz5JuNCCTpETH8dvC1LNi3kc8O7qTgZBn3qtHn/b8nXeL9X5slEq31Aq31FcANQDKwSSn1d6XUiDbeerFr2n5tMQDmDuw/ayZwwO7fGoCf9BrlkSQiRCAKMwXzUzWaKdnDyassZfa2ZRw4WQ7IYL9A0a7uFkopI9ALyMVaijkGvKGUeroT1ywC0mxepwLFHdh/1qtApt2/sYDbqrKEEFYGg4Er0nvx2MBrMGDg/9u5ks/3aqY/0MSEV1Ywce7nTHhlBdMfaGp1RVLhu9qs2lJKPQvcC+wH3gBu0Vo3KKUigEPAkx285krgKaVUElANTAbuP7tTa31QKVWrlBqjtf4GmAossz+J1roSqLSLtYOhCCGcqWdUAr8ffB3v7l3P/5Zu4YonSqRLfABoT4kkGZiktR6rtV6ktW4AaG4ov6OjF9RaHwF+D6wGtgMLtdYblVJLlVLDmg+7E3hFKZUHRAJ/6eh1hBCeEREcys/6XsE1SQNJHVJCU731MSNd4v1XmyUSrfX9F9m3ojMX1VovBBbabZtk8/MOzm+AF0J4qZYa040GAzf3voSy3YmsN31DSGQ9+z7PlS7xfsp3h6QKITyurcb0B36cwlMjJpIRlsSomRtpGLJR1jjxQ349RYoQwnXaO79cVlo4s1Kv4vOD37L08HccPH2c+3tfRoqfr3ESSKREIoTolNbnl7vwWKPByI3Na5xU1NUwe/tyNpUddGu8wnUkkXi5sjLYtAnpMim8ju38ctC+xvR+8ek8PuQ6MiJieTvvGxbmb6LB3OSWeIXrSNWWF5P1TYQ36+z8cvGhETzSf7zNGifl3Nf7MpLDo9wTuHA6SSReStY3Eb6gszPnnl3jpFdMEvP3bmD2tmVM7TWSYUk9XBqvcA2p2vJSHal/FsKTkpJg+PDOzaM1MCGDxwdPJK1LDG/lfcOCfRulqssHSSLxUp2pfxbCFyWERfDYgGuYkNGHr4/m8/z2/+PomZOeDkt0gFRteSlZ30QEEpPRyOTMweTGJPOu3sBz25YzJWc4o1IyPR2aaAcpkXgxWd9EBJr+8V15YshEukfG8+7e9byr11Pb1ODpsEQbpETi5WR9ExFo4kK78MsBV/PvQ7tYeug79p+yLufbPTLe06GJVkiJRAjhdUwGIzf0GMCv+l9NfVMjz29fwcojeZ1egVHGY7mWJBIhhNfKjU3hiSGT6Befzof7tzLnuy+pqq/p0DlkcS3Xk0QihPBqkcGhPNBnLFOyh7Ov6hh/2LqUb08cadd7bcdjyeJariOJRAjh1crKYPNmA32De/G7QdcSExLOa999xaL8TW3OJCzjsdxDEkk7SP2qEJ5hXy311ecx/HbQtYxLV3xZso/nti3n0OkTrb5fxmO5h/TaaoPMdyWEZ1xsmqBbs4fSP74r8/eu5/ntK7ihR38mZPTBaDj/u7GMx3IPSSQXIfNdCeE5rVdLWdd87xOXyqwhk/ggfyOfFO5g54li7s0dRZLd5I+dnQ9MtJ/bE4lSqjvwAda14DVwp9b6tN0xPYBdQEHzplKt9bVuDZS2P8hCCNexrZY6+0XOvloqIjiU+3tfxn/LClmcv5k/bF3GzVmDGZuag8Hwfc2BjMdyLU+USN4A3tBaL1ZKPQE8AfzG7phhwEKt9XS3R2ejPR9kIYRrtLdaymAwMCo5k9yYZN7f+18W5G9iW/lhpuaOJD40wjPBu1FZmfVLrydLW25NJEqpYOBy4EfNm+YDX3FhIhkO9FNKbQdOAA9rrb91V5xnSf2qEJ7VkWqp+NAIftHvKr4u2ceSA9t4estSbs0awuiUrPNKJ/7EW9pw3V0iSQROaq3P9tkrATJaOK4Wa/XXPOA64FOlVB+tdf3ZA5RSsUCs3ftaOpdDpH5VCM/qSLWU0WDgyvRcLolL5729G3h/33/ZUn6Iu3JGEB/mX6UTb2rDdVkiUUrdArxit3kfYD+s1Gz/Xq31UzYvlyql/gj0AXbYbJ8JPOl4pG2T+lUhfEtSeCS/GjCOL4v38knhdp7a+m8mZ1rbTox+UjrxpjZclyUSrfWHwIe225qrto4rpUxa6yYgDSi2f69S6iGsbSTHmzcZAPspQF/FWjVmKwNY43j0QghfZzQYuLqrYkBCV/6+778szN/E5rKD3JUzgpQu0Z4Oz2He1Ibr1qotrXWDUmoNcBuwELgbWNbCoVcA4cCLSqkrABOQZ3euSqDSdptSyhVhCyF8WGJYJDP7Xc03pQV8tH8bz2xdyg+692NCRh+CjCZPh9dp3tSG64leWz8D3lNKPQ4cAu4AUErNANK11rOAh4H5Sqm7gRrgDq31BVVgQgjRHgaDgctSc+gf35V/FGzhs4M72VR2kDtzRpAT47v11t7ShmuwdHJaZm+klOoJHFi1ahUZGU5vdxdC+Imdx4+wsGATFXVnGJ2SxeTMQUQGh3k6LI8pKipi3LhxAJla68KOvl9GtgshAs6AhK6o2BT+99C3rDySx47jRdzUcxBjUrMumGZFtE3umBAiIIWagpicOZgnBk8kvUssH+Rv5I/bV7D/ZLmnQ/M5kkiEEAEtPSKWRwaM4/+p0Zysr+GFHSuYr9dTWXfG06H5DKnaEkIEPIPBwIjkngyI78rSw9+x8kgeW8sPc223vlzTtTchJnlUXozcHSGEaBYWFMyPMwcxNjWbJQe286+DO1lzNJ8f9RzIiKSePjeY0V3zcEnVlhBC2EkKj2JG37E80n8cUcGhvKvXM3vbMr6rKMZXerq6c616KZEIIUQrcmNT+J9B17G57CCfHdzBX3Z9iYpJ4caeA8iO9t7xJ+6eh0sSiRBCXISxuf1kcGI31pTks/Twd7y44z/0i0vjhh4D6REV7+kQL+DuebikaksIIdoh2Gji6q6K2cNv4Kaegzhw6jjPbV/Oa999yYFT3tVl2N1r1UuJRAghOiDUFMR13fpyRVovvijWrDqSx/PbV9A3NpWJ3S6hV0yy29c/sW9Ud/c8XJJIhBCiE8KDgvlB936MS1d8VbKP/xzJ4+VvV9EzKoFrM/owKCHDLaPkW1vcyp3zcEkiEUIIB4QFBXNtt75clZ7L+mMH+E/RHubtWUtSWCRXpucyJiWL8KAQl1y7rUZ1d62lJIlECCGcIMQUxBVpvRibms228iJWFefx4f6t/KtwJ5emZHJFWi/SI+wXdXWMtyxuJYlECCGcyGgwMjSpO0OTunPw1AlWF2vWHi3gy5J9ZEcnMjY1h6GJ3Z0yWt5bFreSRCKEEC7SIyqee9SlTM4czIZjB1hzNJ/5ezewuGALQxO7MSo5k5yY5E6PmPeWxa0kkQghhItFhYRxTUYfxnftzb6qY6w7doDN5Yf4pnQ/CaER1hJMYnd6RMZ3uMeXNyxuJYlECCHcxGAwkBubQm5sCndkD2P78cNsPFbIyiN5rCjaQ0JoBIMTuzEgvis50UmYjO3r9eWuRvXWSCIRQog2uGLyw1BTECOTMxmZnEl1Qx3bjxextfwQXxbvZeWRPLoEBdM3No0+cWn0jU0lPizCORd2AY8lEqXUH4AmrfVTLewLAd4BhmFds32K1jrPvREKIUTr4zScKSI4lDGp2YxJzaa2sYHdlUfZeeII350oZnP5IQCSw6PIjUkmOzqJnOgkksIi3T7wsTVuTyRKqRjgT8AdwIutHPYLoFpr3UcpdTkwHxjlngiF8G/umlrcH7h78kOwjksZktiNIYndsFgsFJ+pYk/lUfIqj7K1/BBrjxYAEBEUSvfIOHpExtMtMo70LjEkh0cRZDS5JrCL8ESJ5EZgH/DyRY75ATALQGv9tVIqSSnVXWt9yB0BCuGv3PHt2p94epyGwWCga0QsXSNiGd+1N2aLhaNnqsg/WUbhqRMcOn2CFUf2YG6e2t6IgaTwKJLDI0kIjSAxLJK40C5Eh4QTHRxGVHAY4UFBTh9x7/ZEorV+H0Ap9dRFDksHSmxelwAZwLlEopSKBexH92Q4J0oh/I8nvl37Om8Zp3GW0WAgPSKW9IhYLk+zbmswN1Fypqr530lKzlRRXnuafVVl1DY1tHieUFMQ4aZggowmTAYDNeWVDsXlskSilLoFeMVuc57Wenw73m4EbFdhMQBmu2NmAk92PkIhAounv137Im8Zp3ExwUYT3SPj6R554XT21Q31VNaf4WR9LScbajjVUEdNYz01TQ3UNjbSYG6iyWLmxOmWE057uSyRaK0/BD7s5NuLgDSgoPl1KlBsd8yrWNtObGUAazp5TSH8mrd9u/YV3jBOo7MigkOICA6haxsdvoqiinjXget4a/ffpcDdwFql1GVArX37iNa6EjivPKaUcl+EQvgYX/h27a08PU7D23lNIlFKzQDStdazgDnAPKXUd0AdMNWjwQnhJ3z527XwXh5LJPbjR7TWc21+rgV+4u6YhAgE8u1aOJsstSuEEMIhkkiEEMJLlJXBpk3W//oSSSRCCI/y1Yensy1abCY7t5HJ95wiO7eRRYstbb/JS0giEUJ4jC8/PFvTmcRoO1h04tzPmfDKCqY/0OQzyVUSiRDCI3z94dmSzibG1geLui5WZ/Ka7r9CiMDibyPtHZmCxtcHi0oiEUJ4hK8/PO05khhdMVjUnbM8SyIRQniEv420dzQxOnOwqLtneZZEIoTwGH8aae+MxOiMwaKemOVZEokQwqP8aaS9NyRGT7Q9SSIRQggn8nRi9ETbkyQSIYTwI55oe5JEIoQQfsbdVWySSIQQwg+5s4pNRrYLIYRwiCQSIYQQDpFEIoQQwiGSSIQQQjjEY43tSqk/AE32S+427+sB7AIKmjeVaq2vdWN4Qggh2sntiUQpFQP8CbgDeLGVw4YBC7XW090WmBBCiE7xRInkRmAf8PJFjhkO9FNKbQdOAA9rrb91R3BCCCE6xu2JRGv9PoBS6qmLHFYLfADMA64DPlVK9dFa1589QCkVC8TavS/DudEKIYRoi8sSiVLqFuAVu815Wuvxbb3Xrt1kqVLqj0AfYIfN9pnAk47GKYQQwjEuSyRa6w+BDzvzXqXUQ1jbSI43bzIADXaHvQrMt9uWAazpzDWFEEJ0jrdOkXIFEA68qJS6AjABebYHaK0rgUrbbUoptwUohBDCymsSiVJqBpCutZ4FPAzMV0rdDdQAd2itzR4NUAghRIs8lkjsx49orefa/HwEuMbdMQkhhOg4GdkuhBDCIZJIhBBCOEQSiRBCCIdIIhFCCOEQSSRCCCEcIolECCGEQySRCCGEcIgkEiGEEA6RRCKEEMIhkkiEEEI4RBKJEEIIh0giEUII4RBJJEIIIRwiiUQIIYRDJJEIIYRwiCQSIYQQDpFEIoQQwiGSSIQQQjhEEokQQgiHuH3NdqXUGOAVIAQ4DvxUa33Q7pgQ4B1gGFADTNFa57k7ViGEEG3zRIlkATBNaz2o+ee/tHDML4BqrXUfYCYw333hCSGE6Ai3lkiUUqHA41rrnc2bdgIPtXDoD4BZAFrrr5VSSUqp7lrrQzbnigVi7d7XA+Do0aNOj10IIfyVzTPT1Jn3uzWRaK3rgA8AlFJG4Cng0xYOTQdKbF6XABnAIZttM4EnW7rOnXfe6YRohRAi4PQCCjr6JpclEqXULVjbQmzlaa3HN7eBvNd8/edaeLsRsNi8NgBmu2Ne5cIqryxgFXAF5yedQJQBrAHGAkUejsXT5F58T+7F9+RefK878BWwvzNvdlki0Vp/CHxov10pFQn8C2tD+41a64YW3l4EpPF9ZkwFiu3OXwlU2p377I+HtNaFDoTv82zuRZHcC7kXZ8m9+J7ci+/Z3Iv6zrzfE43tHwD5wG3NVV0tWQrcDaCUugyotW0fEUII4T3c3dg+GLgR2A1sbc6CxVrrSUqpGUC61noWMAeYp5T6DqgDprozTiGEEO3n7sb2bVjbO1raN9fm51rgJ+6KSwghROf528j2SuBp7NpOApTci+/Jvfie3Ivvyb34nkP3wmCxWNo+SgghhGiFv5VIhBBCuJkkEiGEEA5x+6SNzqKUmgI8DgQDr2qtX7fbPwh4G4gGvgZmaK0b3R6oG7TjXtyItf7TABwA7tVaV7g9UDdo617YHPcD4DWtdaY743OndnwuFDAPiAOOArcH6udCKTUE670IAQ4DdzWPVfNLSqloYB3wQ/sxNJ15dvpkiUQp1RWYDVwGDALuV0r1tTvsA+BBrXUu1gfofe6N0j3auhfNH5i/Aj/QWg/EOr/ZUx4I1eXa+blAKZUCvEQrPQj9QTs+FwasA4Ofb/5cbAN+64lYXa2dn4s/A7Oa74UGHnVvlO6jlBoJrAVyWzmkw89On0wkwHjgC631Ca11NfARcPPZnUqpHkC41npD86b5wC1uj9I9LnovsH4D+7nW+kjz651Yp0PwR23di7PexlpC82dt3YshWGfYXt78+jmgxdKbH2jP58KE9Rs4QBesy1f4q/uAn2M3Wwh0/tnpq1VbLU3qOKKN/RluiMsTLnovtNbHgU8AlFLhWL91znFngG7U1ucCpdQvgK3ABvxbW/ciBziqlHoHGAzsoeWZuP1Bm58L4FfACqXUq0A1MNJNsbmd1noanDctiq1OPTt9tUTS1qSO7Zn00V+063dVSsUA/wZ2aK3fc1Ns7nbRe6GU6gdMBv7g5rg8oa3PRRBwJfBXrfUQrJP1/clt0blXW5+LcKwL6Y3XWqcBbwDvuzVC79GpZ6evJpKzkzqeZT+pY1v7/Umbv6tSKg3rLKc7gWnuC83t2roXtzTv34x1Prd0pdQa94XnVm3di6PAPq315ubXi7jwW7q/aOte9ANqtNYbm1/Pw5pkA1Gnnp2+mkhWAuOaF7zqgvVb5tm6XpqX7q1tXtYXrHN1LXN/mG5x0XuhlDIBnwP/1FrP1Fr78wjUtj4XT2qtc5tX55yEdZ63sR6K1dUuei+w9thJUkoNbH59PbDFzTG6S1v3Ih/opr6v67kR2OTmGL1CZ5+dPplImhuOfw+sBrYDC7XWG5VSS5VSw5oPuxN4RSmVB0TS8pK+Pq8d9+IGrA2rNyultjf/e9uDIbtMOz8XAaGte6G1rgFuAt5qnhz1auARz0XsOu24FxXAPcA/lVI7gZ8C93osYA9w9NkpU6QIIYRwiE+WSIQQQngPSSRCCCEcIolECCGEQySRCCGEcIgkEiGEEA6RRCKEEMIhkkiEEEI4xFcnbRTCZyilfgLMAgZincdoM/BHrXWgzuck/IwMSBTCDZRSC4AqIBRo0lrf7+GQhHAaKZEI4R4zgB1Y17kY6uFYhHAqaSMRwj1SgDAgFuuaD0L4DanaEsLFlFLBWGfbnYf1y9s0YIzWusGjgQnhJFIiEcL1ngNKtdZva63fBMqxriEuhF+QEokQQgiHSIlECCGEQySRCCGEcIgkEiGEEA6RRCKEEMIhkkiEEEI4RBKJEEIIh0giEUII4RBJJEIIIRzy/wNXPwxqZ5kDNwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_samples = 50\n",
    "# Set randomness so that we all get the same answer\n",
    "np.random.seed(42)\n",
    "\n",
    "def true_function(X):\n",
    "    return np.sin(1.5 * np.pi * X)\n",
    "\n",
    "def plot_example(X, Y, functions):\n",
    "    # Get some X's to plot the functions\n",
    "    X_test = pd.DataFrame(np.linspace(0, 1, 100), columns=['X'])\n",
    "    # Plot stuff\n",
    "    for key in functions:\n",
    "        plt.plot(X_test, functions[key](X_test), label=key)\n",
    "    plt.scatter(X, Y, edgecolor='b', s=20, label=\"Samples\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((0, 1))\n",
    "    plt.ylim((-2, 2))\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "# Add X in the range of [0, 1]\n",
    "X = pd.DataFrame(np.sort(np.random.rand(num_samples)), columns=['x1'])\n",
    "# Add some random noise to the observations\n",
    "Y = true_function(X.x1) + np.random.randn(num_samples) * 0.5\n",
    "# Plot stuff\n",
    "functions = {\"True function\": true_function}\n",
    "plot_example(X, Y, functions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that we don't know the true function, choosing to model our noisy observations using linear regression.  (Recall that we built linear regression models in Class #1; compare with the fitting of models for binary target variables from last class.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Fit linear model\n",
    "model = LinearRegression()\n",
    "model.fit(X, Y)\n",
    "# Evaluate model with mean squared error; just as an example\n",
    "mse = mean_squared_error(Y, model.predict(X))\n",
    "# Plot results\n",
    "functions[\"Model\"] = model.predict\n",
    "plot_example(X, Y, functions)\n",
    "#Note how you can customize your plots\n",
    "plt.title(\"Linear Model\\n MSE: %.2f\" % mse)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the linear regression doesn't fit our data super well. Rather than trying a linear regression, let's attempt polynomial regression. How do different degree polynomials fit the data? Recall that a polynomial on a single variable looks like:\n",
    "\n",
    "$$ a_1 + a_2 x + a_3 x^2 + ... $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def fit_polynomial(X, Y, degree):\n",
    "    # create different powers of X\n",
    "    polynomial_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features), (\"linear_regression\", linear_regression)])\n",
    "    pipeline.fit(X, Y)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_poly(X, Y, degree):\n",
    "    # Fit polynomial model\n",
    "    model = fit_polynomial(X, Y, degree)\n",
    "    # Evaluate model\n",
    "    mse = mean_squared_error(Y, model.predict(X))\n",
    "    # Plot results\n",
    "    functions[\"Model\"] = model.predict\n",
    "    plt.title(\"Degree %d\\n MSE: %.2f\" % (degree, mse))\n",
    "    plot_example(X, Y, functions)\n",
    "    \n",
    "plot_poly(X, Y, degree=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to fit our data better than the purely linear model. What if we use polynomials with higher degrees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "# degrees of the polynomial\n",
    "degrees = [1, 4, 15, 30, 50]\n",
    "for i in range(len(degrees)):\n",
    "    ax = plt.subplot(1, len(degrees), i + 1)\n",
    "    plt.setp(ax, xticks=(), yticks=())\n",
    "    plot_poly(X, Y, degrees[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you see there as the effect of allowing more complexity in the modeling process? Take a loot at what happens when we use a regression tree on data generated from the true function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "Expected_Y = true_function(X.x1)\n",
    "plt.figure(figsize=(14, 5))\n",
    "# Fit Regression Trees\n",
    "depths = [1, 2, 5]\n",
    "for i, depth in enumerate(depths):\n",
    "    ax = plt.subplot(1, len(depths), i + 1)\n",
    "    plt.setp(ax, xticks=(), yticks=())    \n",
    "    model = DecisionTreeRegressor(max_depth=depth)\n",
    "    model.fit(X, Expected_Y)\n",
    "    functions = {\"True function\": true_function, \"Tree (depth {})\".format(depth): model.predict}\n",
    "    plot_example(X, Expected_Y, functions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting wine quality\n",
    "\n",
    "_\"All wines should be tasted; some should only be sipped, but with others, drink the whole bottle.\"_ - Paulo Coelho, Brida\n",
    "\n",
    "We will use a data set related to the red variant of the Portuguese \"Vinho Verde\" wine. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.). Our goal is to use machine learning to detect above-average wines (perhaps to send these wines later to professional tasters?).\n",
    "\n",
    "Let's start by loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/winequality.csv\"\n",
    "wine_df = pd.read_csv(path).dropna()\n",
    "# We will change the label to reflect our decision problem, namely, to identify above-average wines.\n",
    "avg_quality = wine_df.quality.mean()\n",
    "wine_df[\"is_good\"] = wine_df.quality > avg_quality\n",
    "#Note above the \"Pandas\" way of doing things: process all the instances simultaneously\n",
    "#   computing the mean in one swoop; assigning the new column all at once.\n",
    "\n",
    "#Now we will get rid of the old feature quality.\n",
    "#  Ask yourself: what would have happened if had used it in predicting the new target?\n",
    "#    (Hint: leakage)\n",
    "wine_df = wine_df.drop(\"quality\", axis=\"columns\")\n",
    "# Replace white spaces for underscores in column names\n",
    "wine_df.columns = [c.replace(' ', '_') for c in wine_df.columns]\n",
    "# Get column names and predictor columns\n",
    "column_names = wine_df.columns\n",
    "predictor_columns = column_names[:-1]\n",
    "wine_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if any of the features seem to be very predictive by themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 4\n",
    "cols = 3\n",
    "fig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(5*cols, 6*rows))\n",
    "axs = axs.flatten()\n",
    "for i in range(len(predictor_columns)):\n",
    "        wine_df.boxplot(predictor_columns[i], by=\"is_good\", grid=False, ax=axs[i], sym='k.')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's no single feature that can separate the data perfectly. Alcohol and total sulfur dioxide look important though. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-structured models\n",
    "Let's now re-explore the modeling technique we introduced last class -- tree-structured models.  And in particular, classification trees, since our target is to predict whether the wine is good or not (binary classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustration, we will increase the complexity of the tree using the maximum depth allowed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "X = wine_df[predictor_columns]\n",
    "Y = wine_df.is_good\n",
    "\n",
    "def plot_trees(X, Y, col1, col2, depths, show_probs=False):\n",
    "    ncol = 3\n",
    "    nrows = np.ceil(len(depths) / ncol)\n",
    "    plt.figure(figsize=[15, 7*nrows])\n",
    "\n",
    "    for i in range(len(depths)):\n",
    "        depth = depths[i] \n",
    "        # Plot\n",
    "        plt.subplot(nrows, ncol, 1+i)\n",
    "        model = DecisionTreeClassifier(max_depth=depth, criterion=\"entropy\")\n",
    "        Decision_Surface(X, col1, col2, Y, model, sample=0.1, gridsize=100,probabilities=show_probs)\n",
    "        plt.title(\"Decision Tree Classifier (max depth=\" + str(depth) + \")\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_trees(X, Y, \"alcohol\", \"total_sulfur_dioxide\", depths=[1,2,3], show_probs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trees are non-linear models\n",
    "\n",
    "If you experiment with the tree depth, you will see that you can fit the data better and better. Deeper trees chop the instance space into smaller and smaller pieces.  Check it out below with the `depths` variable. (Will this finer and finer segmentation go on forever?)\n",
    "\n",
    "**Extra:** Can you visualize the actual tree-structured model?  Hint: there's a function to do it in last week's notebook.  [Caveat: Visualizing huge trees isn't so effective.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_trees(X, Y, \"alcohol\", \"total_sulfur_dioxide\", depths=[1,2,3,4,5,6,10,20,30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear discriminant models\n",
    "\n",
    "Chapter 4 introduces linear models.  Let's try building one on this data set. \n",
    "\n",
    "Looking at the data (see scatterplots above), can you estimate by eye where a good linear discriminant would be?\n",
    "\n",
    "If you remember, linear regression looks like this:\n",
    "\n",
    "$$ y = b + a_1 x_1 + a_2 x_2 + a_3 x_3 + ... $$\n",
    "\n",
    "If you are estimating the probability between two different classes, traditional linear regression may not work as well as you hope. Probabilities need to be bounded between zero and one. To solve this problem, a common tool is to use a **logistic regression**.  Chapter 4 describes it. You can also find logistic regression modeling in the sklearn package.\n",
    "\n",
    "Let's plot both of them together to see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def plot_linear(X, Y, col, model_type, ymin=-0.1, ymax=1.1, sample=1):\n",
    "    if model_type == \"Linear Regression\":\n",
    "        model = LinearRegression()\n",
    "        predict_fn = model.predict\n",
    "    else:\n",
    "        model = LogisticRegression()\n",
    "        predict_fn = lambda obs: model.predict_proba(obs)[:, 1]\n",
    "    title = model_type + \" Regression\"\n",
    "    # Fit model\n",
    "    col_min = X[col].min()\n",
    "    col_max = X[col].max()\n",
    "    col_df = pd.DataFrame(X[col], columns=[col])\n",
    "    model.fit(col_df, Y)\n",
    "    # Evaluate predictions\n",
    "    Y_pred = predict_fn(col_df)\n",
    "    mse = mean_squared_error(Y, Y_pred)\n",
    "    # Plot prediciton line\n",
    "    col_line = pd.DataFrame(np.linspace(col_min, col_max, 100), columns=[col])\n",
    "    plt.plot(col_line, predict_fn(col_line))\n",
    "    # Plot sample\n",
    "    indices = np.random.permutation(range(len(Y)))[:int(sample*len(Y))].tolist()\n",
    "    plt.scatter(col_df[col][indices], Y[indices], edgecolor='b')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Good?\")\n",
    "    plt.xlim((col_min, col_max))\n",
    "    plt.ylim((ymin, ymax))\n",
    "    plt.title(\"%s, MSE %0.3f\" % (title, mse))\n",
    "    \n",
    "def linear_predict(model, X):\n",
    "    return model.predict(X)\n",
    "\n",
    "def logistic_predict(model, X):\n",
    "    return model.predict_proba(X)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15,7])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_linear(X, Y, \"alcohol\", \"Linear Regression\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plot_linear(X, Y, \"alcohol\", \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, of course, we can look at the decision surface produced by linear and logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def training_accy(X, y, model):\n",
    "    y_hat = model.fit(X, y).predict(X)\n",
    "    return accuracy_score(y, [1 if ty > 0.5 else 0 for ty in y_hat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15,7])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "Decision_Surface(X, \"alcohol\", \"total_sulfur_dioxide\", Y, LinearRegression(), sample=0.1, probabilities=False)\n",
    "lin_accy = training_accy(X[[\"alcohol\", \"total_sulfur_dioxide\"]], Y, LinearRegression())\n",
    "plt.title(\"Linear Regression, Accy: %0.3f\" % lin_accy)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "Decision_Surface(X, \"alcohol\", \"total_sulfur_dioxide\", Y, LogisticRegression(), sample=0.1, probabilities=False)\n",
    "lr_accy = training_accy(X[[\"alcohol\", \"total_sulfur_dioxide\"]], Y, LogisticRegression())\n",
    "plt.title(\"Logistic Regression, Accy: %0.3f\" % lr_accy)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating Probabilities\n",
    "\n",
    "\n",
    "For many business problems, we don't need just to estimate the categorical target variable, but we want to estimate the probability that a particular value will be taken. Just about every classification model can also tell you the estimated probability of class membership.\n",
    "\n",
    "Intuitively, how would you generate probabilities from a classification tree? From a linear discriminant?\n",
    "\n",
    "Let's look at the probabilities estimated by these models. As shown below, you can visualize the probabilities both for the linear model and the tree-structured model. Note that the native `LinearRegression` class in sklearn doesn't have probability estimation capability (Why do you think?). We can only perform this operation with logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15,7])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "depth=5\n",
    "model = DecisionTreeClassifier(max_depth=depth, criterion=\"entropy\")\n",
    "Decision_Surface(X, \"alcohol\", \"total_sulfur_dioxide\", Y, model, sample=0.1, probabilities=True)\n",
    "plt.title(\"Decision tree with depth \" + str(depth))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "model = LogisticRegression()\n",
    "Decision_Surface(X, \"alcohol\", \"total_sulfur_dioxide\", Y, model, sample=0.1, probabilities=True)\n",
    "plt.title(\"Logistic regression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's revisit the deeper and deeper trees from above, but this time visualizing the probabilities.  \n",
    "\n",
    "(Do the probabilities for the last trees look odd? )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trees(X, Y, \"alcohol\", \"total_sulfur_dioxide\", depths=[1,2,3,4,5,6,10,20,30], show_probs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linear numeric models\n",
    "\n",
    "Tree-structured models are non-linear, and can fit the data very well. It seems like a linear model possibly cannot. Can we use the mechanism of fitting linear models to generate non-linear boundaries with logistic regression?\n",
    "\n",
    "Yes! We can do this by adding non-linear features, such as  $ x^2 $  or  $ x^3 $ for any feature $ x $. We can even include a full set of polynomial feature interactions: given input features $x_1$ and $x_2$, we can, for instance,  build models and prediction on $x_1 + x_2 + x_1^2 + x_2^2 + x_1x_2$.\n",
    "\n",
    "This is one of the most common ways of introducing non-linearity into numeric function modeling: use a linear function learner, but introduce non-linear features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_model(model=LogisticRegression(), degree=1):\n",
    "    polynomial_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features), (\"model\", model)])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15,7])\n",
    "\n",
    "degrees = [1,2,3]\n",
    "for i in range(len(degrees)):\n",
    "    model = polynomial_model(LogisticRegression(), degrees[i])\n",
    "    plt.subplot(1, len(degrees), i+1) \n",
    "    Decision_Surface(X, \"alcohol\", \"total_sulfur_dioxide\", Y, model, probabilities=True, sample=0.1)\n",
    "    accy = training_accy(X, Y, model)\n",
    "    plt.title(\"Degree %d, Accy: %0.3f\" % (degrees[i], accy))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model is better in this case?? Look at the **accuracy** of each one. Accuracy is simply the count of correct decisions divided by the total number of decisions. Here we are computing the accuracy of the model when it makes predictions on the training set, examples the model \"already knows the answer to\". \n",
    "\n",
    "[From sklearn documentation on sklearn.metrics.accuracy_score: \"In multilabel classification, this function computes subset accuracy: the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true.\"  [More about the accuracy measure..](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization\n",
    "\n",
    "Our evaluation above actually was not what we really want.\n",
    "\n",
    "What we want are models that **generalize** to data that were not used to build them! In other words, we want this model to be able to predict the target for new data instances! Do we know how well our models generalize? Why is this important?\n",
    "\n",
    "<img src=\"images/generalization.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "Let's apply this concept to our data. Now, before we fit out models, we set aside some data to be used later for testing ('holdout data').  This allows us to assess whether the model simply fit the training dataset well, or whether it truly fit some regularities in the domain. \n",
    "\n",
    "Let's use sklearn to set aside some randomly selected holdout data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set randomness so that we all get the same answer\n",
    "np.random.seed(42)\n",
    "shuffled_df = wine_df.sample(frac=1)\n",
    "X = shuffled_df[predictor_columns]\n",
    "Y = shuffled_df.is_good\n",
    "# Split the data into train and test pieces for both X and Y\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=10)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "print ( \"Accuracy on training = %.4f\" % accuracy_score(model.predict(X_train), Y_train) )\n",
    "print ( \"Accuracy on test = %.4f\" % accuracy_score(model.predict(X_test), Y_test) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy on the training set is better than on the test set! Why is this? What can we do to make things better? What happens if our tree gets even deeper? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fitting_curve(datasets, maxdepth=15):\n",
    "    # Intialize accuracies\n",
    "    accuracies = {}\n",
    "    for key in datasets:\n",
    "        accuracies[key] = []\n",
    "    # Initialize depths\n",
    "    depths = range(1, maxdepth+1)\n",
    "    # Fit model for each specific depth\n",
    "    for md in depths:\n",
    "        model = DecisionTreeClassifier(max_depth=md, random_state=42)\n",
    "        # Record accuracies\n",
    "        for key in datasets:\n",
    "            X = datasets[key]['X']\n",
    "            Y = datasets[key]['Y']\n",
    "            if key == \"X-Val\":\n",
    "                accuracies[key].append(cross_val_score(model, X, Y, scoring=\"accuracy\", cv=5).mean())\n",
    "            else:\n",
    "                model.fit(datasets['Train']['X'], datasets['Train']['Y'])\n",
    "                accuracies[key].append(accuracy_score(model.predict(X), Y))\n",
    "    # Plot each curve\n",
    "    plt.figure(figsize=[10,7])\n",
    "    for key in datasets:\n",
    "        plt.plot(depths, accuracies[key], label=key)\n",
    "    # Plot details\n",
    "    plt.title(\"Performance on train and test data\")\n",
    "    plt.xlabel(\"Max depth\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    # Find minimum accuracy in all runs\n",
    "    min_acc = np.array(list(accuracies.values())).min()\n",
    "    plt.ylim([min_acc, 1.0])\n",
    "    plt.xlim([1, maxdepth])\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "datasets = {\"Train\": {\"X\": X_train, \"Y\": Y_train}, \"Test\": {\"X\": X_test, \"Y\": Y_test}}\n",
    "plot_fitting_curve(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "Above, we made a single train/test split. We set aside 20% of our data and *never* used it for training. We also never used the 80% of the data set aside for training to test generalizability.  Although this is far better than testing on the training data, which does not measure generalization performance at all, there are two potential problems with the simple holdout approach.\n",
    "\n",
    "1) Perhaps the random split was particularly bad (or good).  Do we have any confidence in our accuracy estimate?\n",
    "\n",
    "2) We are using only 20% of the data for testing.  Could we possibly use the data more fully for testing?\n",
    "\n",
    "3) Often we want to know something about the distribution of our evaluation metrics. A simple train/test split only allows a single \"point estimate\"\n",
    "\n",
    "Instead of only making the split once, let's use \"cross-validation\" -- every record will contribute to testing as well as to training.\n",
    "\n",
    "\n",
    "<img src=\"images/cross.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=10)\n",
    "scores = cross_val_score(model, X, Y, scoring=\"accuracy\", cv=10)\n",
    "\n",
    "print (\"Cross Validated Accuracy: %0.3f +/- %0.3f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add this cross-validated accuracy to our plot above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"X-Val\"] = {\"X\": X, \"Y\": Y}\n",
    "plot_fitting_curve(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular example, the tree is not overfitting on the test set. However, take a look at the Homework example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "path = \"./data/data-hw1.csv\"\n",
    "np.random.seed(42)\n",
    "df = pd.read_csv(path)\n",
    "# Shuffle data\n",
    "# Get features and label\n",
    "columns = [\"GRE Score\", \"TOEFL Score\", \"University Rating\", \"SOP\", \"LOR\", \"CGPA\", \"Research\"] \n",
    "X = df[columns]\n",
    "Y = df[\"Chance of Admit\"] > 0.5\n",
    "# Split the data into train and test pieces for both X and Y\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20)\n",
    "# Test for overfitting\n",
    "datasets = {\"Train\": {\"X\": X_train, \"Y\": Y_train}, \n",
    "            \"Test\": {\"X\": X_test, \"Y\": Y_test},\n",
    "            \"X-Val\": {\"X\": X, \"Y\": Y}}\n",
    "plot_fitting_curve(datasets, maxdepth=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
