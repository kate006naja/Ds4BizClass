{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Classifiers\n",
    "\n",
    "Term 1 2019 - Instructor: Teerapong Leelanupab\n",
    "\n",
    "Teaching Assistant: Suttida Satjasunsern\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key task when applying a classifier is to determine how effective our classifier will be at making predictions. One way to estimate this is to divide the full dataset into two sets using a \"hold-out strategy\":\n",
    "1. *Training set*: A set of examples used to build the classification model.\n",
    "2. *Test set*: A separate set of examples that is withheld from the classifier during training, and is used afterwards to evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Simple Train/Test Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate how to evaluate classifiers in scikit-learn, we will randomly generate an artificial dataset with 400 examples described by 10 features, annotated with 2 classes: Positive (1) and Negative (-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "data, target = make_hastie_10_2(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.45936383, -0.01534159, -1.78911457,  0.45812823, -0.03201433,\n",
       "        1.13804367, -0.5278233 , -1.10860896,  0.22043537,  0.3348967 ])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.shape)\n",
    "data[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.45936383, -0.01534159, -1.78911457,  0.45812823, -0.03201433,\n",
       "         1.13804367, -0.5278233 , -1.10860896,  0.22043537,  0.3348967 ],\n",
       "       [-0.75795659, -1.34308119,  0.72057333, -0.65958994, -1.20833129,\n",
       "        -0.45483804, -0.77665157, -0.96312656,  0.2326806 , -0.56812524],\n",
       "       [ 0.74034954,  0.29369466,  1.08873705, -0.31758993, -1.0471465 ,\n",
       "        -0.34017487, -0.78892892,  0.67615451, -0.44667061, -0.21387939],\n",
       "       [-0.97335541,  0.03331356, -1.01965619,  0.90629496, -1.15466746,\n",
       "         0.79024203, -1.30368906, -1.60406674,  0.65359115, -0.67078714],\n",
       "       [-0.13840536, -1.99894361, -0.44249836,  0.61801621, -0.33669973,\n",
       "         0.67166099, -0.70427787,  0.79217237, -0.80378995,  1.59405991]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each item in this artificial dataset has a label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1.  1.  1. -1. -1.  1. -1.  1.  1. -1. -1. -1.  1.  1. -1. -1.\n",
      " -1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1.  1.  1.\n",
      "  1. -1. -1.  1.  1.  1.  1. -1.  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.\n",
      " -1.  1.  1. -1. -1.  1.  1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1.  1. -1. -1. -1. -1.  1.  1.  1. -1.  1. -1.  1.\n",
      "  1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.  1. -1.  1.  1.  1.  1.\n",
      "  1.  1. -1.  1. -1. -1. -1.  1. -1.  1.  1. -1. -1.  1. -1.  1.  1. -1.\n",
      "  1.  1. -1.  1.  1. -1. -1. -1.  1.  1. -1. -1.  1. -1. -1. -1.  1.  1.\n",
      " -1.  1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1. -1. -1. -1. -1.  1. -1.\n",
      "  1. -1. -1.  1.  1. -1.  1.  1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.\n",
      "  1.  1.  1. -1. -1.  1. -1.  1.  1.  1. -1. -1. -1.  1. -1. -1.  1.  1.\n",
      "  1. -1.  1.  1.  1. -1.  1.  1. -1.  1.  1.  1.  1.  1. -1. -1. -1. -1.\n",
      "  1.  1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1. -1.  1. -1.  1.  1.  1.\n",
      "  1.  1.  1.  1. -1. -1.  1. -1.  1. -1.  1. -1. -1. -1.  1.  1.  1. -1.\n",
      " -1. -1.  1. -1. -1.  1.  1.  1. -1. -1. -1. -1. -1. -1.  1.  1. -1. -1.\n",
      " -1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1.  1.  1. -1. -1. -1. -1.\n",
      "  1.  1.  1. -1. -1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1. -1.\n",
      " -1. -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.  1.  1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1.  1. -1. -1.  1.  1.  1. -1. -1. -1. -1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1.  1.  1.  1. -1.\n",
      "  1.  1.  1.  1. -1. -1. -1.  1. -1. -1. -1. -1. -1.  1.  1.  1. -1.  1.\n",
      " -1. -1.  1. -1.  1.  1. -1.  1. -1.  1. -1. -1. -1.  1.  1.  1.  1. -1.\n",
      " -1.  1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily randomly split the complete dataset into a training test and a test set. We will specify that 20% (0.2) of the data will be used for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test, target_train, target_test = train_test_split(data, target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1., -1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1.,\n",
       "        1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1.,\n",
       "       -1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1., -1., -1.,  1., -1.,\n",
       "        1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1., -1., -1., -1.,\n",
       "        1.,  1.,  1., -1.,  1., -1., -1., -1., -1.,  1., -1., -1., -1.,\n",
       "       -1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
       "       -1.,  1.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 320 examples\n",
      "Test set has 80 examples\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set has %d examples\" % data_train.shape[0] )\n",
    "print(\"Test set has %d examples\" % data_test.shape[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build a KNN classifier ($k=3$) as we have seen previously. Note that we only use the training data to build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
      "           weights='uniform')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(data_train, target_train)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target [ 1.  1.  1.  1. -1.  1.  1.  1. -1. -1. -1.  1.  1.  1. -1. -1.  1.  1.\n",
      "  1.  1.  1.  1.  1. -1.  1. -1. -1. -1.  1.  1.  1.  1. -1. -1.  1. -1.\n",
      " -1.  1. -1.  1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1.  1.  1.\n",
      "  1. -1.  1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.  1. -1.  1. -1.\n",
      "  1.  1.  1.  1. -1.  1. -1.  1.]\n",
      "Predictions [-1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1.  1.  1.  1. -1. -1.  1.  1.\n",
      " -1.  1.  1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1.\n",
      " -1.  1. -1. -1.  1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.  1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      " -1.  1. -1. -1. -1.  1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict(data_test)\n",
    "print(\"Target\",target_test)\n",
    "print(\"Predictions\",predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually comparing the target labels for the test data with our predictions can be misleading. Instead, we want to determine the extent to which the classifier made the following correct/incorrect predictions:\n",
    "- *True Positives* (TP) are those which are labeled ``1`` which are actually ``1``\n",
    "- *False Positives* (FP) are those which are labeled ``1`` which are actually ``-1``\n",
    "- *True Negatives* (TN) are those which are labeled ``-1`` which are actually ``-1``\n",
    "- *False Negatives* (FN) are those which are labeled ``-1`` which are actually ``1``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this by creating a confusion matrix for the results. The result is a NumPy matrix, with predictions on the columns and actual labels on the rows. The values correspond to:\n",
    "\n",
    "    [ [TP FN]\n",
    "    [FP TN] ]\n",
    "A perfect classifier with 100% accuracy would produce a pure diagonal matrix which would have all the test examples predicted in their correct class. In our case, we see that we have many false negatives (i.e. examples labelled -1 which are actually 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18 27]\n",
      " [ 1 34]]\n"
     ]
    }
   ],
   "source": [
    "# import all of the scikit-learn evaluation functionality\n",
    "from sklearn.metrics import *\n",
    "# build the confusion matrix\n",
    "cm = confusion_matrix(target_test, predicted,labels=[1,-1])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An overall *accuracy* score for the predictions, defined as the fraction of correct predictions, can be calculated using the below. This will return a value between 0 (completely wrong) and 1 (predictions are 100% accurate):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.65\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = %.2f\" % accuracy_score(target_test, predicted) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Measures from information retrieval (search engines) can be used in ML evaluation. Note that these are calculated with respect to a particular class (e.g. the positive class labelled as \"1\").\n",
    "- *Precision*: proportion of retrieved results that are relevant = TP/(TP+FP)\n",
    "- *Recall*: proportion of relevant results that are retrieved = TP/(TP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision (Positive) = 0.95\n",
      "Recall (Positive) = 0.40\n"
     ]
    }
   ],
   "source": [
    "# Note that we indicate that we are interested in the Positive class here, which is labelled as \"1\"\n",
    "print(\"Precision (Positive) = %.2f\" % precision_score(target_test, predicted, pos_label=1) )\n",
    "print(\"Recall (Positive) = %.2f\" % recall_score(target_test, predicted, pos_label=1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is often a trade-off between precision and recall. We can combine precision and recall into a single score using the *F1 Measure*, which is a weighted average of the precision and recall. The F1 Measure reaches its best value at 1 and worst at 0.\n",
    "\n",
    "    F1 = 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 (Positive) = 0.56\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 (Positive) = %.2f\" % f1_score(target_test, predicted, pos_label=1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly compute a summary of these statistics using scikit-learn's provided convenience function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.97      0.71        35\n",
      "    positive       0.95      0.40      0.56        45\n",
      "\n",
      "   micro avg       0.65      0.65      0.65        80\n",
      "   macro avg       0.75      0.69      0.64        80\n",
      "weighted avg       0.78      0.65      0.63        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(target_test, predicted, target_names=[\"negative\",\"positive\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem with simply randomly splitting a dataset into two sets is that each random split might give different results. We are also ignoring a portion of your dataset. One way to address this is to use *k-fold cross-validation* to evaluate a classifier:\n",
    "1. Divide the data into k disjoint subsets - “folds” (e.g. k=5).\n",
    "2. For each of k experiments, use k-1 folds for training and the selected one fold for testing.\n",
    "3. Repeat for all k folds, average the accuracy/error rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is a relatively complex process, scikit-learn allows us to achieve this using a single command. Let's do a 2-fold cross-validation of the KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.645 0.625]\n"
     ]
    }
   ],
   "source": [
    "# create a single classifier\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# apply 2-fold cross-validation, measuring accuracy each time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "acc_scores = cross_val_score(model, data, target, cv=2, scoring=\"accuracy\")\n",
    "print(acc_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, for 10-fold cross validation we get an array with 10 accuracy scores, one for each fold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.625 0.625 0.7   0.525 0.625 0.65  0.7   0.6   0.55  0.625]\n"
     ]
    }
   ],
   "source": [
    "acc_scores =  cross_val_score(model, data, target, cv=10, scoring=\"accuracy\")\n",
    "print(acc_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the average accuracy across all folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN: Mean cross-validation accuracy = 0.62\n"
     ]
    }
   ],
   "source": [
    "print(\"KNN: Mean cross-validation accuracy = %.2f\" % acc_scores.mean() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this approach to compare different classifiers on the same data, such as a logistic regression classifier or a Support Vector Machine (SVM) classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: Mean cross-validation accuracy = 0.47\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "model = linear_model.LogisticRegression(solver='liblinear')\n",
    "acc_scores =  cross_val_score(model, data, target, cv=10, scoring=\"accuracy\")\n",
    "print(\"Logistic Regression: Mean cross-validation accuracy = %.2f\" % acc_scores.mean() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: Mean cross-validation accuracy = 0.91\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC(gamma='scale')\n",
    "acc_scores = cross_val_score(model, data, target, cv=10, scoring=\"accuracy\")\n",
    "print(\"SVM: Mean cross-validation accuracy = %.2f\" % acc_scores.mean() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
