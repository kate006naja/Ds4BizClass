{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Linear Discriminant Models and Estimating Probabilities\n",
    "\n",
    "\n",
    "Term 1 2019 - Instructor: Teerapong Leelanupab\n",
    "\n",
    "Teaching Assistant: Suttida Satjasunsern\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the libraries we will be using\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='ticks', palette='Set2')\n",
    "\n",
    "# some custom libraries!\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from ds_utils.decision_surface import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we're importing library code that we've developed just for this class. In the future, new common code will continue to be added to the `ds_utils` folder. Have some code you use frequently? Consider adding it to that folder as your own library!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivational example\n",
    "\n",
    "Imagine we have some noisy observations from a nonlinear function. We're going to approximate that function by fitting a polynomial to the observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_samples = 50\n",
    "# Set randomness so that we all get the same answer\n",
    "np.random.seed(42)\n",
    "\n",
    "def true_function(X):\n",
    "    return np.sin(1.5 * np.pi * X)\n",
    "\n",
    "def plot_example(X, Y, functions):\n",
    "    # Get some X's to plot the functions\n",
    "    X_test = pd.DataFrame(np.linspace(0, 1, 100), columns=['X'])\n",
    "    # Plot stuff\n",
    "    for key in functions:\n",
    "        plt.plot(X_test, functions[key](X_test), label=key)\n",
    "    plt.scatter(X, Y, edgecolor='b', s=20, label=\"Samples\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((0, 1))\n",
    "    plt.ylim((-2, 2))\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "# Add X in the range of [0, 1]\n",
    "X = pd.DataFrame(np.sort(np.random.rand(num_samples)), columns=['x1'])\n",
    "# Add some random noise to the observations\n",
    "Y = true_function(X.x1) + np.random.randn(num_samples) * 0.5\n",
    "# Plot stuff\n",
    "functions = {\"True function\": true_function}\n",
    "plot_example(X, Y, functions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that we don't know the true function, choosing to model our noisy observations using linear regression.  (Recall that we built linear regression models in Class #1; compare with the fitting of models for binary target variables from last class.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Fit linear model\n",
    "model = LinearRegression()\n",
    "model.fit(X, Y)\n",
    "# Evaluate model with mean squared error; just as an example\n",
    "mse = mean_squared_error(Y, model.predict(X))\n",
    "# Plot results\n",
    "functions[\"Model\"] = model.predict\n",
    "plot_example(X, Y, functions)\n",
    "#Note how you can customize your plots\n",
    "plt.title(\"Linear Model\\n MSE: %.2f\" % mse)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smaller of the square error, the closer you are to finding the best line to fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the linear regression doesn't fit our data super well. Rather than trying a linear regression, let's attempt polynomial regression. How do different degree polynomials fit the data? Recall that a polynomial on a single variable looks like:\n",
    "\n",
    "$$ a_1 + a_2 x + a_3 x^2 + ... $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def fit_polynomial(X, Y, degree):\n",
    "    # create different powers of X\n",
    "    polynomial_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features), (\"linear_regression\", linear_regression)])\n",
    "    pipeline.fit(X, Y) # fit using x, y\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_poly(X, Y, degree):\n",
    "    # Fit polynomial model\n",
    "    model = fit_polynomial(X, Y, degree)\n",
    "    # Evaluate model\n",
    "    mse = mean_squared_error(Y, model.predict(X))\n",
    "    # Plot results\n",
    "    functions[\"Model\"] = model.predict\n",
    "    plt.title(\"Degree %d\\n MSE: %.2f\" % (degree, mse))\n",
    "    plot_example(X, Y, functions)\n",
    "    \n",
    "plot_poly(X, Y, degree=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to fit our data better than the purely linear model. What if we use polynomials with higher degrees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "# degrees of the polynomial\n",
    "degrees = [1, 4, 7, 15, 30, 50]\n",
    "for i in range(len(degrees)):\n",
    "    ax = plt.subplot(1, len(degrees), i + 1)\n",
    "    plt.setp(ax, xticks=(), yticks=())\n",
    "    plot_poly(X, Y, degrees[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you see there as the effect of allowing more complexity in the modeling process? Take a loot at what happens when we use a regression tree on data generated from the true function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "Expected_Y = true_function(X.x1)\n",
    "plt.figure(figsize=(14, 5))\n",
    "# Fit Regression Trees\n",
    "depths = [1, 2, 5]\n",
    "for i, depth in enumerate(depths):\n",
    "    ax = plt.subplot(1, len(depths), i + 1)\n",
    "    plt.setp(ax, xticks=(), yticks=())    \n",
    "    model = DecisionTreeRegressor(max_depth=depth)\n",
    "    model.fit(X, Expected_Y)\n",
    "    functions = {\"True function\": true_function, \"Tree (depth {})\".format(depth): model.predict}\n",
    "    plot_example(X, Expected_Y, functions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting wine quality\n",
    "\n",
    "_\"All wines should be tasted; some should only be sipped, but with others, drink the whole bottle.\"_ - Paulo Coelho, Brida\n",
    "\n",
    "We will use a data set related to the red variant of the Portuguese \"Vinho Verde\" wine. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.). Our goal is to use machine learning to detect above-average wines (perhaps to send these wines later to professional tasters?).\n",
    "\n",
    "Let's start by loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"data/winequality.csv\"\n",
    "wine_df = pd.read_csv(path).dropna()\n",
    "# We will change the label to reflect our decision problem, namely, to identify above-average wines.\n",
    "avg_quality = wine_df.quality.mean()\n",
    "wine_df[\"is_good\"] = wine_df.quality > avg_quality # above mean(average)\n",
    "#Note above the \"Pandas\" way of doing things: process all the instances simultaneously\n",
    "#   computing the mean in one swoop; assigning the new column all at once.\n",
    "\n",
    "#Now we will get rid of the old feature quality.\n",
    "#  Ask yourself: what would have happened if had used it in predicting the new target?\n",
    "#    (Hint: leakage)\n",
    "wine_df = wine_df.drop(\"quality\", axis=\"columns\")\n",
    "# Replace white spaces for underscores in column names\n",
    "wine_df.columns = [c.replace(' ', '_') for c in wine_df.columns]\n",
    "# Get column names and predictor columns\n",
    "column_names = wine_df.columns\n",
    "predictor_columns = column_names[:-1] # not include last column(cause they want to predict I guess)\n",
    "wine_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if any of the features seem to be very predictive by themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows = 4\n",
    "cols = 3\n",
    "fig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(5*cols, 6*rows))\n",
    "axs = axs.flatten() # Return a copy of the array collapsed into one dimension.\n",
    "for i in range(len(predictor_columns)):\n",
    "        wine_df.boxplot(predictor_columns[i], by=\"is_good\", grid=False, ax=axs[i], sym='k.')\n",
    "        \n",
    "# Automatically adjust subplot parameters to give specified padding.\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's no single feature that can separate the data perfectly. \n",
    "#### Alcohol and  total sulfur dioxide look important though. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-structured models\n",
    "Let's now re-explore the modeling technique we introduced last class -- tree-structured models.  And in particular, classification trees, since our target is to predict whether the wine is good or not (binary classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustration, we will increase the complexity of the tree using the maximum depth allowed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "X = wine_df[predictor_columns]\n",
    "Y = wine_df.is_good\n",
    "\n",
    "def plot_trees(X, Y, col1, col2, depths, show_probs=False):\n",
    "    ncol = 3\n",
    "    nrows = np.ceil(len(depths) / ncol)\n",
    "    plt.figure(figsize=[15, 7*nrows])\n",
    "\n",
    "    for i in range(len(depths)):\n",
    "        depth = depths[i] \n",
    "        # Plot\n",
    "        plt.subplot(nrows, ncol, 1+i) # cause it using len so it must plus 1 every i\n",
    "        model = DecisionTreeClassifier(max_depth=depth, criterion=\"entropy\")\n",
    "        #Decision_Surface(X, col1, col2, Y, model, sample=0.1, gridsize=100,probabilities=show_probs)\n",
    "        Decision_Surface(X, col1, col2, Y, model,probabilities=show_probs)\n",
    "        plt.title(\"Decision Tree Classifier (max depth=\" + str(depth) + \")\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_trees(X, Y, \"alcohol\", \"total_sulfur_dioxide\", depths=[1,2,3], show_probs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trees are non-linear models\n",
    "\n",
    "If you experiment with the tree depth, you will see that you can fit the data better and better. Deeper trees chop the instance space into smaller and smaller pieces.  Check it out below with the `depths` variable. (Will this finer and finer segmentation go on forever?)\n",
    "\n",
    "**Extra:** Can you visualize the actual tree-structured model?  Hint: there's a function to do it in last week's notebook.  [Caveat: Visualizing huge trees isn't so effective.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_trees(X, Y, \"alcohol\", \"total_sulfur_dioxide\", depths=[1,2,3,4,5,6,10,20,30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear discriminant models\n",
    "\n",
    "Chapter 4 introduces linear models.  Let's try building one on this data set. \n",
    "\n",
    "Looking at the data (see scatterplots above), can you estimate by eye where a good linear discriminant would be?\n",
    "\n",
    "If you remember, linear regression looks like this:\n",
    "\n",
    "$$ y = b + a_1 x_1 + a_2 x_2 + a_3 x_3 + ... $$\n",
    "\n",
    "If you are estimating the probability between two different classes, traditional linear regression may not work as well as you hope. Probabilities need to be bounded between zero and one. To solve this problem, a common tool is to use a **logistic regression**.  Chapter 4 describes it. You can also find logistic regression modeling in the sklearn package.\n",
    "\n",
    "Let's plot both of them together to see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def plot_linear(X, Y, col, model_type, ymin=-0.1, ymax=1.1, sample=1):\n",
    "    if model_type == \"Linear Regression\":\n",
    "        model = LinearRegression()\n",
    "        predict_fn = model.predict\n",
    "    else:\n",
    "        model = LogisticRegression()\n",
    "        predict_fn = lambda obs: model.predict_proba(obs)[:, 1]\n",
    "    title = model_type + \" Regression\"\n",
    "    # Fit model\n",
    "    col_min = X[col].min()\n",
    "    col_max = X[col].max()\n",
    "    col_df = pd.DataFrame(X[col], columns=[col])\n",
    "    model.fit(col_df, Y)\n",
    "    # Evaluate predictions\n",
    "    Y_pred = predict_fn(col_df)\n",
    "    mse = mean_squared_error(Y, Y_pred)\n",
    "    # Plot prediciton line\n",
    "    col_line = pd.DataFrame(np.linspace(col_min, col_max, 100), columns=[col])\n",
    "    plt.plot(col_line, predict_fn(col_line))\n",
    "    # Plot sample\n",
    "    indices = np.random.permutation(range(len(Y)))[:int(sample*len(Y))].tolist()\n",
    "    plt.scatter(col_df[col][indices], Y[indices], edgecolor='b')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Good?\")\n",
    "    plt.xlim((col_min, col_max))\n",
    "    plt.ylim((ymin, ymax))\n",
    "    plt.title(\"%s, MSE %0.3f\" % (title, mse))\n",
    "    \n",
    "def linear_predict(model, X):\n",
    "    return model.predict(X)\n",
    "\n",
    "def logistic_predict(model, X):\n",
    "    return model.predict_proba(X)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15,7])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_linear(X, Y, \"alcohol\", \"Linear Regression\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plot_linear(X, Y, \"alcohol\", \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, of course, we can look at the decision surface produced by linear and logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def training_accy(X, y, model):\n",
    "    y_hat = model.fit(X, y).predict(X)\n",
    "    return accuracy_score(y, [1 if ty > 0.5 else 0 for ty in y_hat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15,7])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "Decision_Surface(X, \"alcohol\", \"total_sulfur_dioxide\", Y, LinearRegression(), sample=0.1, probabilities=False)\n",
    "lin_accy = training_accy(X[[\"alcohol\", \"total_sulfur_dioxide\"]], Y, LinearRegression())\n",
    "plt.title(\"Linear Regression, Accy: %0.3f\" % lin_accy)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "Decision_Surface(X, \"alcohol\", \"total_sulfur_dioxide\", Y, LogisticRegression(), sample=0.1, probabilities=False)\n",
    "lr_accy = training_accy(X[[\"alcohol\", \"total_sulfur_dioxide\"]], Y, LogisticRegression())\n",
    "plt.title(\"Logistic Regression, Accy: %0.3f\" % lr_accy)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating Probabilities\n",
    "\n",
    "\n",
    "For many business problems, we don't need just to estimate the categorical target variable, but we want to estimate the probability that a particular value will be taken. Just about every classification model can also tell you the estimated probability of class membership.\n",
    "\n",
    "Intuitively, how would you generate probabilities from a classification tree? From a linear discriminant?\n",
    "\n",
    "Let's look at the probabilities estimated by these models. As shown below, you can visualize the probabilities both for the linear model and the tree-structured model. Note that the native `LinearRegression` class in sklearn doesn't have probability estimation capability (Why do you think?). We can only perform this operation with logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15,7])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "depth=5\n",
    "model = DecisionTreeClassifier(max_depth=depth, criterion=\"entropy\")\n",
    "Decision_Surface(X, \"alcohol\", \"total_sulfur_dioxide\", Y, model, sample=0.1, probabilities=True)\n",
    "plt.title(\"Decision tree with depth \" + str(depth))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "model = LogisticRegression()\n",
    "Decision_Surface(X, \"alcohol\", \"total_sulfur_dioxide\", Y, model, sample=0.1, probabilities=True)\n",
    "plt.title(\"Logistic regression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's revisit the deeper and deeper trees from above, but this time visualizing the probabilities.  \n",
    "\n",
    "(Do the probabilities for the last trees look odd? )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_trees(X, Y, \"alcohol\", \"total_sulfur_dioxide\", depths=[1,2,3,4,5,6,10,20,30], show_probs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linear numeric models\n",
    "\n",
    "Tree-structured models are non-linear, and can fit the data very well. It seems like a linear model possibly cannot. Can we use the mechanism of fitting linear models to generate non-linear boundaries with logistic regression?\n",
    "\n",
    "Yes! We can do this by adding non-linear features, such as  $ x^2 $  or  $ x^3 $ for any feature $ x $. We can even include a full set of polynomial feature interactions: given input features $x_1$ and $x_2$, we can, for instance,  build models and prediction on $x_1 + x_2 + x_1^2 + x_2^2 + x_1x_2$.\n",
    "\n",
    "This is one of the most common ways of introducing non-linearity into numeric function modeling: use a linear function learner, but introduce non-linear features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def polynomial_model(model=LogisticRegression(), degree=1):\n",
    "    polynomial_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features), (\"model\", model)])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15,7])\n",
    "\n",
    "degrees = [1,2,3]\n",
    "for i in range(len(degrees)):\n",
    "    model = polynomial_model(LogisticRegression(), degrees[i])\n",
    "    plt.subplot(1, len(degrees), i+1) \n",
    "    Decision_Surface(X, \"alcohol\", \"total_sulfur_dioxide\", Y, model, probabilities=True, sample=0.1)\n",
    "    accy = training_accy(X, Y, model)\n",
    "    plt.title(\"Degree %d, Accy: %0.3f\" % (degrees[i], accy))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model is better in this case?? Look at the **accuracy** of each one. Accuracy is simply the count of correct decisions divided by the total number of decisions. Here we are computing the accuracy of the model when it makes predictions on the training set, examples the model \"already knows the answer to\". \n",
    "\n",
    "[From sklearn documentation on sklearn.metrics.accuracy_score: \"In multilabel classification, this function computes subset accuracy: the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true.\"  [More about the accuracy measure..](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
