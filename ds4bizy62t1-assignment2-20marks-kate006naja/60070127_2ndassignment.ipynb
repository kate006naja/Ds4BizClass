{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science for Business 2nd Assignment (Web Scrabbing and Text classification)\n",
    "### Katesopon Kunpanperng 60070127"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to scrabbing text data about News Article in Prof's website, storing it in txt file and do a classification task to see which article is in which category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 : Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First just import all (really all of it) package used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\theka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Data Collection\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting all month link using original link and beautifulsoup scrab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use as a link for access to month and article\n",
    "link = \"http://www.it.kmitl.ac.th/~teerapong/news_archive/\"\n",
    "\n",
    "page = requests.get(\"http://www.it.kmitl.ac.th/~teerapong/news_archive/index.html\")\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Link: http://www.it.kmitl.ac.th/~teerapong/news_archive/month-jan-2017.html\n",
      "Last Link: http://www.it.kmitl.ac.th/~teerapong/news_archive/month-dec-2017.html\n"
     ]
    }
   ],
   "source": [
    "mlist = []\n",
    "for i in soup.find_all('a', href=True):\n",
    "    i = str(i).split('\"')\n",
    "    if i[1] == \"#\":\n",
    "        break\n",
    "    else:\n",
    "        mlist.append(link+i[1])\n",
    "# print first link and last link to confirm\n",
    "print(\"First Link:\", mlist[0]+\"\\n\"+\"Last Link:\", mlist[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collecting all category and store it in pandas series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    technology\n",
       "1      business\n",
       "2    technology\n",
       "3      business\n",
       "4         sport\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define new list to store category\n",
    "catlist = []\n",
    "for i in mlist:\n",
    "    monthpage = requests.get(i)\n",
    "    # access each month page\n",
    "    monthsoup = BeautifulSoup(monthpage.content, \"html.parser\")\n",
    "    for j in monthsoup.find_all(class_=\"category\"):\n",
    "        catlist.append(re.findall(\"[a-z]+\", str(j))[5])\n",
    "# save as Series for easier data manipulate\n",
    "catlist = pd.Series(catlist)\n",
    "catlist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at category values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sport         526\n",
       "business      491\n",
       "technology    391\n",
       "td             53\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catlist.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "oops it got **\"td\"** which is false at first by storing data techniques but if it in website article category its actually mean N/A value or  its mean that there is no article available there right now so I will remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first I will replace it with nan(np.nan)\n",
    "catlist.replace(\"td\", np.nan, inplace=True)\n",
    "\n",
    "# second I will drop it (cause there is no meaning to keep it)\n",
    "catlist = catlist.dropna()\n",
    "\n",
    "# check if it still have null values\n",
    "catlist.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sport         526\n",
       "business      491\n",
       "technology    391\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catlist.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1408\n"
     ]
    }
   ],
   "source": [
    "print(526 + 491 + 391)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'category':catlist})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     category\n",
       "0  technology\n",
       "1    business\n",
       "2  technology\n",
       "3    business\n",
       "4       sport"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store article category to csv file in datastore\n",
    "df.to_csv(\"datastore/cate.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finish collecting and storing article, now it time to access all article link and collect article text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get every link to access article data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.it.kmitl.ac.th/~teerapong/news_archive/article-jan-0418.html',\n",
       " 'http://www.it.kmitl.ac.th/~teerapong/news_archive/article-jan-0027.html',\n",
       " 'http://www.it.kmitl.ac.th/~teerapong/news_archive/article-jan-0631.html']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datalink = []\n",
    "for i in mlist:\n",
    "    monpage = requests.get(i)\n",
    "    monsoup = BeautifulSoup(monpage.content, \"html.parser\")\n",
    "    monsoup = monsoup.find(class_=\"table table-condensed table-striped\")\n",
    "    # find where it match condition in find all\n",
    "    for j in monsoup.findAll('a', attrs={'href': re.compile(\".html$\")}):\n",
    "        datalink.append(link+str(j.get('href')))\n",
    "datalink[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1408"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to make sure that there are same amount of data as category\n",
    "len(datalink)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### access website by datalink and store text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open text file to write data\n",
    "filex = open('datastore/article.txt', 'w', encoding='utf-8')\n",
    "\n",
    "# run for loop in datalink to access to everylink\n",
    "for i in datalink:\n",
    "    pands = requests.get(i)\n",
    "    pands = BeautifulSoup(pands.content, 'html.parser')\n",
    "    # create new string value to store data\n",
    "    eachline = \"\"\n",
    "    \n",
    "    # why not contain first and last ?\n",
    "    # because first is when web tell you article title or just blank\n",
    "    # and last is when web tell you to comment or something kinda that but I dont want it.\n",
    "    for j in pands.find_all('p')[1:-1:1]:\n",
    "        j = j.get_text()\n",
    "        eachline += str(j)\n",
    "\n",
    "    filex.writelines(str(eachline)+\"\\n\")\n",
    "    time.sleep(0.01) # using sleep because website not allow you to run for loop rapidly in a short time\n",
    "\n",
    "filex.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to check if i got same amount of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of Article stored: 1408\n"
     ]
    }
   ],
   "source": [
    "filex = open(\"datastore/article.txt\", \"r\", encoding='utf-8')\n",
    "f1 = filex.readlines()\n",
    "filex.close()\n",
    "print(\"Total amount of Article stored:\",len(f1))\n",
    "# clear output\n",
    "f1 = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see we got exactly same amount of links as amount of category because the non-existing link didn't store in website anymore (compare to NA value in category)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 : Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data from datastore and store it in Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1408 raw text documents.\n"
     ]
    }
   ],
   "source": [
    "# Load Article document\n",
    "file_1 = open(\"datastore/article.txt\", \"r\", encoding='utf-8')\n",
    "raw_documents = file_1.readlines()\n",
    "file_1.close()\n",
    "print(\"There are %d raw text documents.\" % len(raw_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BT is offering customers free internet telephone calls if they sign up to broadband in December.The Christmas give-away entitles customers to free telephone calls anywhere in the UK via the internet. Users will need to use BT\\'s internet telephony software, known as BT Communicator, and have a microphone and speakers or headset on their PC. BT has launched the promotion to show off the potential of a broadband connection to customers.People wanting to take advantage of the offer will need to be a BT Together fixed-line customer and will have to sign up to broadband online. The offer will be limited to the first 50,000 people who sign up and there are limitations - the free calls do not include calls to mobiles, non-geographical numbers such as 0870, premium numbers or international numbers. BT is keen to provide extra services to its broadband customers. \"People already using BT Communicator have found it by far the most convenient way of making a call if they are at their PC,\" said Andrew Burke, director of value-added services at BT Retail. As more homes get high-speed access, providers are increasingly offering add-ons such as cheap net calls. \"Broadband and telephony are attractive to customers and BT wants to make sure it is in the first wave of services,\" said Ian Fogg, an analyst with Jupiter Research. \"BT Communicator had a quiet launch in the summer and now BT is waving the flag a bit more for it,\" he added.BT has struggled to maintain its market share of broadband subscribers as more competitors enter the market. Reports say that BT has lost around 10% of market share over the last year, down from half of broadband users to less than 40%. BT is hoping its latest offer can persuade more people to jump on the broadband bandwagon. It currently has 1.3 million broadband subscribers.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_documents[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1408"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Article Category file\n",
    "category = pd.read_csv(\"datastore/cate.csv\")\n",
    "len(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     category\n",
       "0  technology\n",
       "1    business\n",
       "2  technology\n",
       "3    business\n",
       "4       sport"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to know if the label is balance or imbalance I need to plot bar graph to see if it balance or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(4,3))\n",
    "category.groupby('category').category.count().plot.bar(ylim=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this bar plot im pretty sure that it a balance data so now i will do a train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm not splitting data because it better to try train test using cross validation\n",
    "X = raw_documents\n",
    "Y = category.category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before I go straight tokenize my data and want to define function that use to reduce a term into its canonical form (a bit more advance form of stemming) by using **Lemmatization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Lemmatization Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use this function for being parameter in text tokenizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lemma tization function\n",
    "def lemma_tokenizer(text):\n",
    "    # use the standard scikit-learn tokenizer first\n",
    "    standard_tokenizer = CountVectorizer().build_tokenizer()\n",
    "    tokens = standard_tokenizer(text)\n",
    "    # then use NLTK to perform lemmatisation on each token\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    lemma_tokens = []\n",
    "    for token in tokens:\n",
    "        lemma_tokens.append( lemmatizer.lemmatize(token) )\n",
    "    return lemma_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not do a train test split becuase I will use **cross-validation** to testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorizer with binary vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theka\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(binary=True, stop_words='english', min_df = 2, tokenizer=lemma_tokenizer)\n",
    "count_vectorizer.fit(X)\n",
    "\n",
    "X_binary = count_vectorizer.transform(X)\n",
    "# Im apply dense because dense data is required if im not converting X to dense it will error when applied in model.\n",
    "X_binary = X_binary.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theka\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', min_df = 2, tokenizer=lemma_tokenizer)\n",
    "tfidf_vectorizer.fit(X)\n",
    "\n",
    "X_tfidf = tfidf_vectorizer.transform(X)\n",
    "X_tfidf = X_tfidf.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm using 4 model in this time and then apply these 4 model seperately on each tokenization model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_gnb = GaussianNB() # Gaussian Naive-Bayes\n",
    "model_mltb = MultinomialNB() # Multinomial Naive-Bayes\n",
    "model_comple = ComplementNB() # Complement Naive-Bayes\n",
    "knn = KNeighborsClassifier(n_neighbors=4) # K-Nearest Neighbors\n",
    "models = [model_gnb, model_mltb, model_comple, knn]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I set `n_neighbors = 4` because I test between (3-20) in binary model to see which one got highest scores on cross-validate and 4 got the highest scores(even it not that high)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validate on Binary-Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score from Binary vectorizer and GaussianNB model is: 0.9680232533424024\n",
      "Score Standard Deviation is 0.010728621130922348\n",
      "\n",
      "Score from Binary vectorizer and MultinomialNB model is: 0.977962888175654\n",
      "Score Standard Deviation is 0.012155761430401759\n",
      "\n",
      "Score from Binary vectorizer and ComplementNB model is: 0.9793916721576297\n",
      "Score Standard Deviation is 0.012916856878405278\n",
      "\n",
      "Score from Binary vectorizer and KNeighborsClassifier model is: 0.7180970802247398\n",
      "Score Standard Deviation is 0.03532234877423803\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in models:\n",
    "    model = i\n",
    "    scores = cross_val_score(model, X_binary, Y, cv=10)\n",
    "    print(\"Score from Binary vectorizer and\", i.__class__.__name__, \"model is:\", scores.mean())\n",
    "    print(\"Score Standard Deviation is\", str(scores.std())+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validate  on TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score from TF-IDF vectorizer and GaussianNB model is: 0.9431494746388361\n",
      "Score Standard Deviation is 0.027652661047592016\n",
      "\n",
      "Score from TF-IDF vectorizer and MultinomialNB model is: 0.9765294634443571\n",
      "Score Standard Deviation is 0.015327113120478731\n",
      "\n",
      "Score from TF-IDF vectorizer and ComplementNB model is: 0.9808250968889265\n",
      "Score Standard Deviation is 0.014615670523954197\n",
      "\n",
      "Score from TF-IDF vectorizer and KNeighborsClassifier model is: 0.9609112164431313\n",
      "Score Standard Deviation is 0.0111661212985854\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in models:\n",
    "    model = i\n",
    "    scores = cross_val_score(model, X_tfidf, Y, cv=10)\n",
    "    print(\"Score from TF-IDF vectorizer and\", i.__class__.__name__, \"model is:\", scores.mean())\n",
    "    print(\"Score Standard Deviation is\", str(scores.std())+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying many models and two of tokenizer I better say that I would like to use **TF-IDF tokenizer** and **Multinomial Naive-Bayes Algorithm** because I want to avoid some word that appear in every document and Multinomial is work well for multinomial distributed data and TF-IDF and why I don't use ComplementNB even it got higher score is becuase ComplementNB work well with imbalance data. So that why I think MultinomialNB is **suitable** to use in this task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
